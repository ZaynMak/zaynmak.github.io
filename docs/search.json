[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Error with mostly Complete Penguins.\n\n\n\n\n\n\nMay 22, 2023\n\n\nZayn Makdessi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression without Bikeshare.\n\n\n\n\n\n\nMay 22, 2023\n\n\nZayn Makdessi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 1\n\n\n\n\n\n\nApr 19, 2023\n\n\nZayn Makdessi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComplete Gradient Descent.\n\n\n\n\n\n\nMar 1, 2023\n\n\nZayn Makdessi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComplete Perceptron.\n\n\n\n\n\n\nFeb 22, 2023\n\n\nZayn Makdessi\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Zayn",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Perceptron/Perceptron.html",
    "href": "posts/Perceptron/Perceptron.html",
    "title": "Blog Post 1: Perceptron",
    "section": "",
    "text": "When fit is called, the perceptron takes random weights and starts looping until the score reaches 1, or we’ve done max_steps number of loops. We then take the dot product of a random observation with the weight, and if it is negative then we add that abservation to the weight, otherwise if the dot product is positive then we subtract the observation from the weight. After that we check the score of our perceptron using the score function, which in turn calls the predict function and then add the result to our history variable.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nfrom perceptron import Perceptron\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\n# We start by generating a random dataset with 2 classes at opposite ends of the graph which makes it very likely for them to be linearly separable\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# We then fit the data to the perceptron\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n# We plot the data\nfig = plt.scatter(X[:,0], X[:,1], c = y)\n# Then plot the decision boundary\nfig = draw_line(p.weight[0], -2, 2)\n\n# And label the axes\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy over time\")\n\n\n\n\n\n# We then generate a random dataset with 2 classes that are not linearly separable by having their centers be very close to each other\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(1.0, 0.9), (1.0, 1.0)])\n\n# We then fit the data to the perceptron\np2 = Perceptron()\np2.fit(X2, y2, max_steps = 1000)\n\n# We plot the data\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\n# Then plot the decision boundary\nfig2 = draw_line(p2.weight[0], -2, 2)\n\n# And label the axes\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p2.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy over time\")\n\n\n\n\n\n# We then generate a random dataset with 5 classes that are not linearly separable by having their centers be at the center and the four corners of the graph\nX3, y3 = make_blobs(n_samples = 100, n_features = 5, centers = [(-1.7, -1.7), (1.7, 1.7), (1.7, -1.7), (-1.7, 1.7), (0, 0)])\n# We then fit the data to the perceptron\np3 = Perceptron()\np3.fit(X3, y3, max_steps = 1000)\nfig = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy over time\")\n# seing how the accuracy does not history reach 1 by the end, it is not linearly separable\n\n\n\n\nThe run time of a single iteration of the perceptron is O(p) because the numbers of observations (data points) do not matter, since we are only doing the dot product and then adding p features to p features."
  },
  {
    "objectID": "posts/Penguins/Penguins.html",
    "href": "posts/Penguins/Penguins.html",
    "title": "Blog Post 3: Classifying Palmer Penguins",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Patch\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n# set seed for reproducibility\nnp.random.seed(0)\n\n\nspecies_group = train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Species\"]].groupby('Species').aggregate('mean')\n# display figure\nsns.scatterplot(data=train, x=\"Culmen Length (mm)\", y=\"Body Mass (g)\", hue=\"Species\").set(title=\"Culmen Length vs Body Mass of Different Penguin Species\")\n# display table\nspecies_group\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n    \n    \n      Species\n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      38.710256\n      18.365812\n      189.965812\n      3667.094017\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      48.719643\n      18.442857\n      195.464286\n      3717.857143\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      47.757000\n      15.035000\n      217.650000\n      5119.500000\n    \n  \n\n\n\n\n\n\n\nFirst I only selected a few columns with quantitative data, then grouped the observations by species and saw the mean of the values. Looking at the table and figure we see that Gentoo penguins tend to have a higher body mass, flipper length, and smaller culmen depth compared to the other two species. Meanwhile Adelie penguins tend to have a smaller culmen length and flipper length compared to the other two species. Chinstrap penguins tend to have a similar body mass and culmen depth to Adelie penguins, and similar culmen length to Gentoo penguins. The figure agrees with our observations of the table. We see that Gentoo penguins tend to have a higher body mass, while Adelie penguins tend to have a smaller culmen length. The figure also shows that Chinstrap penguins tend to have a similar body mass to Adelie penguins, and similar culmen length to Gentoo penguins.\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n# I picked the quantitative and qualitative predictors that I thought would be most useful for predicting the species of a penguin.\nall_qual_cols = [\"Clutch Completion\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\"]\n\nbest_cols = 0\nbest_score = 0\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    LR = LogisticRegression(max_iter=3000)\n    LR.fit(X_train[cols], y_train)\n    score = LR.score(X_train[cols], y_train)\n    if score > best_score:\n      best_score = score\n      best_cols = cols\n\nprint(best_cols, best_score)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'] 0.99609375\n\n\nThe predictors that result in the highest accuracy in a logistic regression are Island, Culmen Length, and Culmen Depth. Next we want to see our model’s accuracy on the test set.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n\nLR = LogisticRegression(max_iter=1000)\nLR.fit(X_train[best_cols], y_train)\nLR.score(X_test[best_cols], y_test)\n\n1.0\n\n\nWe found a 100% accuracy on the test set. This is a very high accuracy, and we can be confident that our model is a good predictor of penguin species. Out of Curiosity, I wanted to see if other predictors would be better on different models. I first ran a random forest classifier, decision tree classifier.\n\nbest_cols = 0\nbest_score = 0\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    RF = RandomForestClassifier(max_depth=5)\n    RF.fit(X_train[cols], y_train)\n    score = RF.score(X_train[cols], y_train)\n    if score > best_score:\n      best_score = score\n      best_cols = cols\n\nprint(best_cols, best_score)\nRF = RandomForestClassifier(max_depth=5)\nRF.fit(X_train[best_cols], y_train)\nRF.score(X_test[best_cols], y_test)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'] 1.0\n\n\n0.9852941176470589\n\n\nFor the random forest classifier, we find the same predictors are used, however the testing accuracy is not 100%.\n\nbest_cols = 0\nbest_score = 0\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    DR = DecisionTreeClassifier(max_depth=5)\n    DR.fit(X_train[cols], y_train)\n    score = DR.score(X_train[cols], y_train)\n    if score > best_score:\n      best_score = score\n      best_cols = cols\n\nprint(best_cols, best_score)\nDR = DecisionTreeClassifier(max_depth=5)\nDR.fit(X_train[best_cols], y_train)\nDR.score(X_test[best_cols], y_test)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'] 1.0\n\n\n0.9852941176470589\n\n\nSimilarly for the decision tree classifier, we find the same predictors are used, however the testing accuracy is not 100%.\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1],\n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[best_cols], y_train)\n\n\n\n\nOn Biscoe island, where we only have Gentoo and Adelie penguins, we see a clear decision line because both of these penguin speciees have dissimilar culmen length and depths. On Dream island, where there are Gentoo and Chinstrap penguins, they have a similar distribution of culmen depth, but a bigger difference in culmen length which allows us to place the decision line between them. On Torgersen, there are only Gentoo penguins, so we the decision lines are placed randomly."
  },
  {
    "objectID": "posts/Gradient-Descent/LogisticRegressiono.html",
    "href": "posts/Gradient-Descent/LogisticRegressiono.html",
    "title": "Blog Post 2: Gradient Descent",
    "section": "",
    "text": "LR = LogisticRegression()\nLR.fit_stochastic(X, y, 0.01, 10, 1000)\n\n# inspect the fitted value of w\nLR.w \n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\nloss = LR.empirical_risk(X_, y)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = plt.plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\ntitle = plt.gca().set_title(f\"Loss = {loss}\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\n\n\n# Experiment 1\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, 5, 10, 1000)\n# LR.loss_history\n\n[0.1069168039367764,\n 0.10797678817511525,\n 0.10636153004008612,\n 0.10714108593373343,\n 0.11141679485021486,\n 0.11124219725437028,\n 0.10246056337629167,\n 0.09997356331501515,\n 0.10927368581696129,\n 0.10337479766047863,\n 0.10041772170517062,\n 0.10843191547799265,\n 0.10352614908275964,\n 0.10496464561911388,\n 0.10220367457737847,\n 0.10554084998542404,\n 0.11699660577231004,\n 0.10783245921055759,\n 0.10366066157221016,\n 0.10054258041142732,\n 0.10807811439075184,\n 0.10245210073942818,\n 0.10473880975008058,\n 0.10631899033124716,\n 0.10031082978876732,\n 0.1011866068883192,\n 0.10129510114303382,\n 0.10000484401527167,\n 0.1004234654060928,\n 0.11056507745501967,\n 0.10581340513584092,\n 0.10764097270533583,\n 0.10697292242186421,\n 0.10560538006307246,\n 0.10009586478686289,\n 0.10095361220463783,\n 0.10402500745027311,\n 0.10474368720299186,\n 0.10021873650534989,\n 0.1047154536865298,\n 0.10258926358795488,\n 0.10136379913029334,\n 0.10344911047562587,\n 0.10269653324287052,\n 0.11251472455711702,\n 0.10466059508516423,\n 0.1013632715578644,\n 0.10532221428628304,\n 0.10037861450312226,\n 0.0999428542092721,\n 0.10036306271654424,\n 0.10169818775064482,\n 0.10787295175956527,\n 0.1027781691342378,\n 0.11048922285205,\n 0.10082881872391689,\n 0.10563847140865,\n 0.10065847372255222,\n 0.1042335079413558,\n 0.103387209034269,\n 0.10244510874675107,\n 0.10844425390593879,\n 0.10158322526789315,\n 0.10050978501280305,\n 0.1010390053664975,\n 0.10040448789966465,\n 0.1059467483456504,\n 0.11021098955283719,\n 0.10965004698526522,\n 0.10093509242913758,\n 0.12265871989138899,\n 0.10000514825338773,\n 0.1001517724430593,\n 0.10614440466197216,\n 0.10157243237860403,\n 0.12842343299119421,\n 0.10216971406243676,\n 0.11706965489002999,\n 0.10255375779605243,\n 0.1087613252797543,\n 0.10651596524746615,\n 0.11887998990840522,\n 0.10006200570142136,\n 0.10248269286798237,\n 0.10493067924292646,\n 0.10686197467078111,\n 0.1163193833972236,\n 0.10433955937049173,\n 0.10329771700020697,\n 0.10088142506432282,\n 0.09970428417484016,\n 0.09996477453354867,\n 0.10932965415582059,\n 0.1014829616012513,\n 0.1132983468746654,\n 0.10082937222286777,\n 0.10767716345695687,\n 0.10299630329321605,\n 0.10078257900629777,\n 0.10286002307627484,\n 0.10334275962317115,\n 0.10213573534129884,\n 0.11144957350871602,\n 0.11685970097129057,\n 0.10353773332115715,\n 0.1029737228972072,\n 0.10080695970719432,\n 0.10034693289286249,\n 0.1052698711942967,\n 0.11733402369813285,\n 0.10369940421319353,\n 0.11280175664834509,\n 0.1023689028708181,\n 0.1061346278411021,\n 0.10132535943724029,\n 0.1011569330361002,\n 0.10533399999673225,\n 0.10703604485943309,\n 0.116823913569469,\n 0.10766154334812744,\n 0.1059830129119476,\n 0.10691188076613439,\n 0.1132307930776438,\n 0.10149425480853756,\n 0.10186091743083538,\n 0.10310571757563292,\n 0.10492522520583598,\n 0.10095976407742054,\n 0.10443513929792701,\n 0.11800466277776461,\n 0.1002935240772832,\n 0.10239502888676034,\n 0.1074673333644149,\n 0.10063334753631455,\n 0.1002380799946792,\n 0.12941480477789147,\n 0.10052041969511855,\n 0.10210739938637194,\n 0.10054167801371587,\n 0.10833578070516542,\n 0.10229083992116529,\n 0.10969794932667988,\n 0.10051320742825742,\n 0.10251471429520301,\n 0.10786997099831958,\n 0.1006128499669159,\n 0.10645201933195288,\n 0.10192625251035223,\n 0.10061225191625578,\n 0.10106651896293194,\n 0.10891082510893625,\n 0.10201814203658163,\n 0.10054248125882374,\n 0.10328755541387163,\n 0.10211134450969508,\n 0.10109863103625118,\n 0.10272117812064603,\n 0.10421847562423898,\n 0.10142066330042397,\n 0.10217772474851977,\n 0.1049534663758881,\n 0.10010042627442645,\n 0.10060230595960826,\n 0.10919688989213526,\n 0.10359649101219234,\n 0.10309699184166045,\n 0.10533384614640927,\n 0.10720643595587012,\n 0.1048721710420189,\n 0.10693797987111847,\n 0.11403112090576353,\n 0.10112529198376403,\n 0.1072702552402183,\n 0.1070624841496048,\n 0.100918354351332,\n 0.10074251969393906,\n 0.10817633631726815,\n 0.10389216573324561,\n 0.10019851952493923,\n 0.1011387957326375,\n 0.10142637676299135,\n 0.10015523216973012,\n 0.1063197894885282,\n 0.10075139327477334,\n 0.10264726813278681,\n 0.10267413529534238,\n 0.10196866169383352,\n 0.10130765900479279,\n 0.10421361907927112,\n 0.10835957405852235,\n 0.10101677763828555,\n 0.10283807151999827,\n 0.10547827272688931,\n 0.10053650242029963,\n 0.11040303832223973,\n 0.10401203074863766,\n 0.11335818806096232,\n 0.1017026038567173,\n 0.1005551093024301,\n 0.10415515851985947,\n 0.12080331069068144,\n 0.10982532583735719,\n 0.10198136556609,\n 0.10059002517219548,\n 0.10982677634738348,\n 0.1021648476932247,\n 0.10291743209422809,\n 0.10216314422828064,\n 0.1416775276840668,\n 0.10188201523503655,\n 0.11632089088719529,\n 0.10031874815781379,\n 0.1115229902400428,\n 0.10500313928528669,\n 0.10130431234318085,\n 0.10089811365290291,\n 0.10375434907599086,\n 0.10080229308170786,\n 0.10636736450862017,\n 0.11434912528803651,\n 0.10243402733304498,\n 0.10494910162765278,\n 0.10619229433721683,\n 0.11665209669078201,\n 0.1028319227598979,\n 0.10226619467691514,\n 0.10438684400053055,\n 0.10411310917095806,\n 0.1099399905616296,\n 0.10721044557692312,\n 0.10420625036143795,\n 0.10427187973118374,\n 0.10019486326584466,\n 0.1102852440038439,\n 0.1035241680395384,\n 0.1063567213523023,\n 0.1033007804426266,\n 0.1296597721842153,\n 0.11094816877336239,\n 0.10323185056119963,\n 0.10546184446210972,\n 0.11025045533290961,\n 0.10124025065475097,\n 0.10873864173315004,\n 0.1022288902698559,\n 0.1001433464541907,\n 0.10067881387801213,\n 0.10158717761866969,\n 0.10223181124041897,\n 0.10149717275516482,\n 0.10062722493648452,\n 0.10541337175500097,\n 0.103470994529244,\n 0.10529540751982673,\n 0.10110906483305655,\n 0.10843269425420475,\n 0.1014854372854996,\n 0.10202848971522166,\n 0.10293127168706077,\n 0.10068343342577979,\n 0.1008408979650067,\n 0.1042523983201392,\n 0.10131562882769705,\n 0.10294721190269301,\n 0.10449559565516503,\n 0.1137204643922685,\n 0.12001769473327549,\n 0.10641171615672157,\n 0.10113483907559641,\n 0.10491567255939077,\n 0.10097985234699067,\n 0.12125361279550823,\n 0.1104099458480115,\n 0.10969068513561085,\n 0.10279350962194872,\n 0.1034200188526818,\n 0.10929481732822406,\n 0.10025230605027506,\n 0.10412485147138195,\n 0.10350326337170618,\n 0.10032551532153,\n 0.10167313907471762,\n 0.10217670716978425,\n 0.10081058649128388,\n 0.10149915534255846,\n 0.10109705805287003,\n 0.10068088981490934,\n 0.10094634556595718,\n 0.10918370423505025,\n 0.10085523783228577,\n 0.11615950016206039,\n 0.10459365572603914,\n 0.11021745223775198,\n 0.10659413665232087,\n 0.10426032396503176,\n 0.10283674177790314,\n 0.10982897439402625,\n 0.11250822251099592,\n 0.09991336173495383,\n 0.11189057026128171,\n 0.10030259863557946,\n 0.11270284577194659,\n 0.1021918255499254,\n 0.10013235754802477,\n 0.10963442832378263,\n 0.10525041628484104,\n 0.11329025271560308,\n 0.11065745150729545,\n 0.10708333440729173,\n 0.10476277512004387,\n 0.10154746298986497,\n 0.11443712276370305,\n 0.10042880640377527,\n 0.10062331506986835,\n 0.11631747647628427,\n 0.10075971265280963,\n 0.10310659320080706,\n 0.11609122406279627,\n 0.10031618961118015,\n 0.10238659733408881,\n 0.10073048741599049,\n 0.10289110441790178,\n 0.1120377068492489,\n 0.1048634496495443,\n 0.10298739619209794,\n 0.10329139131559051,\n 0.11739991745790795,\n 0.09986119254983866,\n 0.102979562168226,\n 0.11369812949542148,\n 0.1148180836431833,\n 0.10701256334361166,\n 0.10264734715820759,\n 0.10228634851262072,\n 0.10331800481781699,\n 0.10308148348347829,\n 0.11372903423587456,\n 0.10466188995623205,\n 0.10225342026318597,\n 0.10108172093622222,\n 0.12053158735370709,\n 0.10375525088166292,\n 0.10574317873150116,\n 0.10259515293845585,\n 0.10399936370252227,\n 0.1009271716524324,\n 0.100999535494702,\n 0.11244449586198027,\n 0.10729391258666858,\n 0.11147677638316368,\n 0.11003353450693477,\n 0.10361382696678799,\n 0.10427214604697838,\n 0.10225886112995212,\n 0.12153959693611754,\n 0.10469844222799354,\n 0.10168834126708359,\n 0.10455829833432181,\n 0.10102347033111293,\n 0.11291728867421852,\n 0.11041437026743935,\n 0.10436530673304961,\n 0.10596025950269347,\n 0.10697094579231958,\n 0.10329772512542622,\n 0.10181882203779238,\n 0.101422102490341,\n 0.10471946725398106,\n 0.1064077338796212,\n 0.10039426760501005,\n 0.10203412435550253,\n 0.10370196542705248,\n 0.10026212507586035,\n 0.10176717449891302,\n 0.10327009641540837,\n 0.12204089798314026,\n 0.1254230449874217,\n 0.10844403964170532,\n 0.10084462382390129,\n 0.10775572512794156,\n 0.1045395666060435,\n 0.10786839058298617,\n 0.10139502323456208,\n 0.11422386383221522,\n 0.1057963171096234,\n 0.10270339121733159,\n 0.10150598767123901,\n 0.10209578072085676,\n 0.10021544020632284,\n 0.10348748310959945,\n 0.1089362874840122,\n 0.10138505593618621,\n 0.10446832927087485,\n 0.10409479741457027,\n 0.10095237694884876,\n 0.10022791023155747,\n 0.1093660521641307,\n 0.1048737445802294,\n 0.10771307712296288,\n 0.10194016449829636,\n 0.10049145945436459,\n 0.10130713963136528,\n 0.1089125376169692,\n 0.11348293119402811,\n 0.10327769670109728,\n 0.10384786738870527,\n 0.10116666339811672,\n 0.100168193661761,\n 0.10145018077254642,\n 0.11675190945417598,\n 0.10330674053814687,\n 0.10038452519008674,\n 0.11202642480728517,\n 0.10484025380071887,\n 0.1225794935047469,\n 0.10807944291221414,\n 0.1001920670967931,\n 0.10565870727013711,\n 0.10933853512121917,\n 0.11674745615572561,\n 0.10077648493504444,\n 0.10241042371757729,\n 0.11096315567421108,\n 0.1044203221847238,\n 0.10642696797237132,\n 0.108746703015455,\n 0.1015525680498497,\n 0.10137360361784775,\n 0.10184196842685153,\n 0.10288301120920598,\n 0.10507748707248445,\n 0.10112475169248342,\n 0.10237837865646993,\n 0.1034893005759464,\n 0.10756273586118038,\n 0.10000044849201598,\n 0.10805564418228983,\n 0.10332619515877915,\n 0.10071066704109534,\n 0.10296340263209607,\n 0.1037416245346793,\n 0.11545341447992451,\n 0.10058278574606355,\n 0.10686291131831042,\n 0.10054595548596104,\n 0.12279585061352374,\n 0.10231680245245905,\n 0.10028743873618912,\n 0.09985442116173654,\n 0.10247215044963091,\n 0.10516032443608035,\n 0.10962449536190816,\n 0.1111816296639122,\n 0.10882098376508534,\n 0.11459293113561579,\n 0.10155299274998442,\n 0.10919107445361,\n 0.10530394477177543,\n 0.10236774028083774,\n 0.11470336066921889,\n 0.10354480945152661,\n 0.1205709185935051,\n 0.11482999197574935,\n 0.10381389787717168,\n 0.1055363803506216,\n 0.10797199395777662,\n 0.11653723305838325,\n 0.10620809782728294,\n 0.10339973522530693,\n 0.1092486796599753,\n 0.10919420482135081,\n 0.10895764582031131,\n 0.10679745147139426,\n 0.10013875751007621,\n 0.1252433241462832,\n 0.1009957110441662,\n 0.10410128418366454,\n 0.10773915475596939,\n 0.10745786035045211,\n 0.10873309794995265,\n 0.10267982218211731,\n 0.10996308146534937,\n 0.10250904533664446,\n 0.10511437363490991,\n 0.10337575658737652,\n 0.10341986280055117,\n 0.10392610771825225,\n 0.10036215705997208,\n 0.1018309580094223,\n 0.10087749036196714,\n 0.10387549594706474,\n 0.10186837243002547,\n 0.09967793894616797,\n 0.10041888982370556,\n 0.09981492446186241,\n 0.09993107562395127,\n 0.10371976703077185,\n 0.10015451531934282,\n 0.10136263376435073,\n 0.10038402593049044,\n 0.10353456436661983,\n 0.10315519439959019,\n 0.10174918598593333,\n 0.11007678264527548,\n 0.12970117148206975,\n 0.10126014174036811,\n 0.12445445267939799,\n 0.10556671731918929,\n 0.10247481452880829,\n 0.10833334649587628,\n 0.10146753450909728,\n 0.1023297553265709,\n 0.10169668511351432,\n 0.10502136172472133,\n 0.10341231691824754,\n 0.10881579961622803,\n 0.09994520549932959,\n 0.11259902887374208,\n 0.10667698492938774,\n 0.10780768867507576,\n 0.1034014359111953,\n 0.11223104100328751,\n 0.10731817996467104,\n 0.10077236443042917,\n 0.10313063340380228,\n 0.10566880826552834,\n 0.1020029383339488,\n 0.10484621843727952,\n 0.10865659910500888,\n 0.10027469064466878,\n 0.10626070369242395,\n 0.10201743576470776,\n 0.1024792239206773,\n 0.10012671578591188,\n 0.10983540539363477,\n 0.10142754473254857,\n 0.12055335705309575,\n 0.10006008987646578,\n 0.11430898121475642,\n 0.10514435737953842,\n 0.1012167107205873,\n 0.10384813525038165,\n 0.1016656468119994,\n 0.10023534832828605,\n 0.10695232356246458,\n 0.10530870941864207,\n 0.10445771349166195,\n 0.11092323949664394,\n 0.10002911960897949,\n 0.10047536680822107,\n 0.10122146704962566,\n 0.10231186650551033,\n 0.10391932735373498,\n 0.1123611084191112,\n 0.10294143061614172,\n 0.10101968606162426,\n 0.10093617925016103,\n 0.12773629042725176,\n 0.10578904045212704,\n 0.10068236926346025,\n 0.11169617311031858,\n 0.10622939295106637,\n 0.10028328662831357,\n 0.10107909122851161,\n 0.10707577883114716,\n 0.10276476791136763,\n 0.10026483713293144,\n 0.11127166943918337,\n 0.11118945133543548,\n 0.10177349823894685,\n 0.10626931151463785,\n 0.10360075072885239,\n 0.10582103580061243,\n 0.10253727866049096,\n 0.10103896563587686,\n 0.10191136215508337,\n 0.10056825611452677,\n 0.10157439602896132,\n 0.10047474707424545,\n 0.10055239825488865,\n 0.1046134426331957,\n 0.11481650769972877,\n 0.10069350095354218,\n 0.10033747987563515,\n 0.11124945253463762,\n 0.10353291716683088,\n 0.10238486149607151,\n 0.11987447996638696,\n 0.10315246073535844,\n 0.11118029528489248,\n 0.1007871048755612,\n 0.10053564597443966,\n 0.1018788016838116,\n 0.10059847045409569,\n 0.10323776476825561,\n 0.10008770559479054,\n 0.10390506486603979,\n 0.10615759913955347,\n 0.10182189054124667,\n 0.11384961979957342,\n 0.10014714970193472,\n 0.11209532924572654,\n 0.10229913626580918,\n 0.10558447288294831,\n 0.10573232800169591,\n 0.11221674781542713,\n 0.10095088765010388,\n 0.1008424220061986,\n 0.11824351577963084,\n 0.10405473154626219,\n 0.10590794883065346,\n 0.12078299545014026,\n 0.11336385224379124,\n 0.10402436484861194,\n 0.10254309177192202,\n 0.10691216779753625,\n 0.10042986038739223,\n 0.10133818672232614,\n 0.10224704640790402,\n 0.10040247462874102,\n 0.09974491930827693,\n 0.10579933050215642,\n 0.10563537413965818,\n 0.10656108215919881,\n 0.12185272343988199,\n 0.10975157065538026,\n 0.10341866160394293,\n 0.11313993832733267,\n 0.10529347218964352,\n 0.10705940376098369,\n 0.11115043275941465,\n 0.10435882038213393,\n 0.10058463915312119,\n 0.10207683399828166,\n 0.10500115589087862,\n 0.10581947745265738,\n 0.1037362053675585,\n 0.10646192858389014,\n 0.10083727256934319,\n 0.10538471412016437,\n 0.10625361778001499,\n 0.10237802643981041,\n 0.10259079319148733,\n 0.10502581900924396,\n 0.10054447647357753,\n 0.10050622516556373,\n 0.10761516757296487,\n 0.10368140918477696,\n 0.10131440444560902,\n 0.10805563038892622,\n 0.1002430573528915,\n 0.10242861469129981,\n 0.10311765753720184,\n 0.10484116787715028,\n 0.1082304577383238,\n 0.10285727114153861,\n 0.11266913195343868,\n 0.10984525001644263,\n 0.10085446643616529,\n 0.10074814521250464,\n 0.10097161725135298,\n 0.1012106963672709,\n 0.10589536374767572,\n 0.10252137354711272,\n 0.10098173950284436,\n 0.10044650787714388,\n 0.10463410159485848,\n 0.10007732582006543,\n 0.10785985621977044,\n 0.09976481367013744,\n 0.10192880638850088,\n 0.10492291590880125,\n 0.10022110577235561,\n 0.10680778791667546,\n 0.11740973558576943,\n 0.11165754261354956,\n 0.10487956372158504,\n 0.10241006623368572,\n 0.10019513281856866,\n 0.10848845691065874,\n 0.11151025596818924,\n 0.10145304491622895,\n 0.1092855029292295,\n 0.10252510041019391,\n 0.11376918431328827,\n 0.10388175234616469,\n 0.10639936462642914,\n 0.10151333724333558,\n 0.10093468063901426,\n 0.10043688984980434,\n 0.10055200708618284,\n 0.1278034520057391,\n 0.10910219792094605,\n 0.10136029358172938,\n 0.10740992629729568,\n 0.10223922061033544,\n 0.1149267974846875,\n 0.10049126695398117,\n 0.11374759692984152,\n 0.11293743649855444,\n 0.1137132413239178,\n 0.10290241747179846,\n 0.1055303466662769,\n 0.11356098413893903,\n 0.10955784170425922,\n 0.10207683023409504,\n 0.10417000791022034,\n 0.11588389932924842,\n 0.10008841959121277,\n 0.1041816913507644,\n 0.10519660808291328,\n 0.10587025531896505,\n 0.10035328081223845,\n 0.10794972287019426,\n 0.10785469948661848,\n 0.10331397509944683,\n 0.10066926879545975,\n 0.10741686314646193,\n 0.10439783613727506,\n 0.11154406262372561,\n 0.10686895514662396,\n 0.10812072585537522,\n 0.10058916956885287,\n 0.1030530008017255,\n 0.10004956098504043,\n 0.10258833524566739,\n 0.10390994705956055,\n 0.10758336397655213,\n 0.1043361106176377,\n 0.10630890124808828,\n 0.10066402802431265,\n 0.1030384577706813,\n 0.10258239576696077,\n 0.10314240135184129,\n 0.10520973692095162,\n 0.12337271019605915,\n 0.1132782436153314,\n 0.10025883502896217,\n 0.10092666307686389,\n 0.10142877138861543,\n 0.11010731017839767,\n 0.10453889967973823,\n 0.1006180046399374,\n 0.1049025036371425,\n 0.10180169336330365,\n 0.10065155544353889,\n 0.10742993452809646,\n 0.11444750722737945,\n 0.09995300833022068,\n 0.10039414016148082,\n 0.1145321311781349,\n 0.10555949629843328,\n 0.09985119130819706,\n 0.1044561455724319,\n 0.10426933810501932,\n 0.10378117300141529,\n 0.11137690359734838,\n 0.10063978175768075,\n 0.10099648585973559,\n 0.1125465241014048,\n 0.10026629910413666,\n 0.10089511381342195,\n 0.10071255376704384,\n 0.10504633823171393,\n 0.11274259796056114,\n 0.10189047226647753,\n 0.10410190361697641,\n 0.10478942924380855,\n 0.1152316611257184,\n 0.10192961627501104,\n 0.1009984957444643,\n 0.10293389571440835,\n 0.10295052260319308,\n 0.10108797192800381,\n 0.10095434137592708,\n 0.1023686971053823,\n 0.10740070325564503,\n 0.10230672522858178,\n 0.1028776396106269,\n 0.10146338464721869,\n 0.10738480342742754,\n 0.10721096766556311,\n 0.11154037360879972,\n 0.11112902119237388,\n 0.10302215013066786,\n 0.10007016685912681,\n 0.1166029806650465,\n 0.11219200252515435,\n 0.10001083565291313,\n 0.14404052601412481,\n 0.12057465597541686,\n 0.10215275390731232,\n 0.11087657356287468,\n 0.10021687621192012,\n 0.09981894031231171,\n 0.10585404829964698,\n 0.10236124388960119,\n 0.10019261145892575,\n 0.10923008468345184,\n 0.10013929310943834,\n 0.11031627838564743,\n 0.1150975253620109,\n 0.10441043366630776,\n 0.10717779657897884,\n 0.11412066859009144,\n 0.10032863773606959,\n 0.10913736915662248,\n 0.10397742190890095,\n 0.11025718664027134,\n 0.10818353236162252,\n 0.10816697448121924,\n 0.10038532485762872,\n 0.1055141962679419,\n 0.10045541932508575,\n 0.10186167693369484,\n 0.10937844993326248,\n 0.1124891019378055,\n 0.1011600825356282,\n 0.10227771748719419,\n 0.10438219367877846,\n 0.10808798446068917,\n 0.10278859460815554,\n 0.11663062273584053,\n 0.10693443185697905,\n 0.1038388378754988,\n 0.10095065600358188,\n 0.10445481904293588,\n 0.10225641598064833,\n 0.1080564296452544,\n 0.10152739956851359,\n 0.10280659291893404,\n 0.10405542800779337,\n 0.10437859567707534,\n 0.10543477563399202,\n 0.10697588372714799,\n 0.10683972363695494,\n 0.12572666407655597,\n 0.10441448684455568,\n 0.1061915826080472,\n 0.10052139485061072,\n 0.10134298752354802,\n 0.11122932035831522,\n 0.10205189166199852,\n 0.12171637629008679,\n 0.10446149567469132,\n 0.10071642107194972,\n 0.11150827644867721,\n 0.10633963089543755,\n 0.10160588985915248,\n 0.10943561891621414,\n 0.11323672676420289,\n 0.10023506363139041,\n 0.10502163150904018,\n 0.10469864133937837,\n 0.10306712871934748,\n 0.11401687354941746,\n 0.10219614617276934,\n 0.10389862565403693,\n 0.10726527835864275,\n 0.12150744558367585,\n 0.10163341642278305,\n 0.11195068310162984,\n 0.10351154705787127,\n 0.10378819187283173,\n 0.10062219939318619,\n 0.10117773950561947,\n 0.1112516270459689,\n 0.10783917615743226,\n 0.10801434440889796,\n 0.10154673544042915,\n 0.1078211032429064,\n 0.10291401975352958,\n 0.10294626101360875,\n 0.1062479702678887,\n 0.11177648278504665,\n 0.1022099427502166,\n 0.1090234274204437,\n 0.10466233247913304,\n 0.10436886422515163,\n 0.1068996199448884,\n 0.108573304791531,\n 0.1006427917418743,\n 0.10606876237433109,\n 0.10300888020621986,\n 0.10482131899792194,\n 0.10061334769670761,\n 0.10501428878523118,\n 0.11760277252822608,\n 0.10427377234452045,\n 0.09990940477376035,\n 0.10071355799322156,\n 0.108951931925628,\n 0.10299372524178027,\n 0.11289194465449034,\n 0.10018012949191071,\n 0.10184892744658779,\n 0.10435891405901854,\n 0.10002154397894064,\n 0.10579866518803087,\n 0.10153360497816614,\n 0.101408761992264,\n 0.10062547689803601,\n 0.10201050988684283,\n 0.10856142133693222,\n 0.10443221512216123,\n 0.10060861976857051,\n 0.11578671243603321,\n 0.10044632018636483,\n 0.11165057937975345,\n 0.10644118716116051,\n 0.10140000300773779,\n 0.10130205907882484,\n 0.10129384792469093,\n 0.1027055871659718,\n 0.09993987907052528,\n 0.10184773614163838,\n 0.10264218800283466,\n 0.10779138000360679,\n 0.10010225622679865,\n 0.10462989050338166,\n 0.10060650991648261,\n 0.1049115499951229,\n 0.10801736033826188,\n 0.10173686208366281,\n 0.10917958177396671,\n 0.10135288006817053,\n 0.11148194427138852,\n 0.12296245748898188,\n 0.10135188112141684,\n 0.10211894225537364,\n 0.11885585570350511,\n 0.10129908185762954,\n 0.10255875013524811,\n 0.10490084310532516,\n 0.10708145009480913,\n 0.10228859788576006,\n 0.10312981191030669,\n 0.10169805747569066,\n 0.10480590370219449,\n 0.1025707127270491,\n 0.10439256253598345,\n 0.1016878830389874,\n 0.11182432780021735,\n 0.11158087045331787,\n 0.10409639632687895,\n 0.10879905192254165,\n 0.10298093200091214,\n 0.10905865918966477,\n 0.10939623347618177,\n 0.1030613526019712,\n 0.10411912533760902,\n 0.10227350625427273,\n 0.10021144707325583,\n 0.10476458667622915,\n 0.10346589099481801,\n 0.11329194131144821,\n 0.10091014760302905,\n 0.11666001794788027,\n 0.10203140515527012,\n 0.11209780932906854,\n 0.1013751334266637,\n 0.1164578114600625,\n 0.10288881808387135,\n 0.10692063678082044,\n 0.1003736894000925,\n 0.10107616463270419,\n 0.10071373208167884,\n 0.10487419245246409,\n 0.10438626855624811,\n 0.11139115269441642,\n 0.10333934308875042,\n 0.10277032114156494,\n 0.10704177764273667,\n 0.10537101664017812,\n 0.10015853066986427,\n 0.10470121563703337,\n 0.11188761165407078,\n 0.1050022411662345,\n 0.10137662049634544,\n 0.10138372706374459,\n 0.1027226319891945,\n 0.10146079027453046,\n 0.10402815098464441,\n 0.10503696881121531,\n 0.10574039304494445,\n 0.11476564136207135,\n 0.10334249947393749,\n 0.10706824961449925,\n 0.10771277869676847,\n 0.10102433741090186,\n 0.10000532873620063,\n 0.10813953546761151,\n 0.10180981558899305,\n 0.10234150776316998,\n 0.10205660837110765,\n 0.09984037630525823,\n 0.10327184279756539,\n 0.11083424280671032,\n 0.10006830501957972,\n 0.11100230575827105]\n\n\n\n# Experiment 2\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 2, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 2\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 3, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 3\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 7, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 7\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 15, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 15\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 30, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 30\")\n\nplt.loglog()\n\nlegend = plt.legend()"
  },
  {
    "objectID": "posts/Gradient-Descent/LogisticRegression.html",
    "href": "posts/Gradient-Descent/LogisticRegression.html",
    "title": "Blog Post 2: Gradient Descent",
    "section": "",
    "text": "from logistic_regression import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nnp.random.seed(1234)\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, 0.01, 10, 1000)\n\n# inspect the fitted value of w\nLR.w \n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n# calculate the loss\nloss = LR.empirical_risk(X_, y)\n\n# plot the decision boundary\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\n# we plot the data with the decision boundary\np = plt.plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\ntitle = plt.gca().set_title(f\"Loss = {loss}\")\n\n\n\n\n\n# we find the loss history for the stochastic gradient descent with momentum\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  alpha = .05,\n                  batch_size = 10) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\n# we find the loss history for the stochastic gradient descent without momentum\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 10, \n                  alpha = .5)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n# we find the loss history for the gradient descent\nLR = LogisticRegression()\nLR.fit(X, y,\n       alpha = .05,\n       max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\n# we set it in the log space\nplt.loglog()\n\n# we set the labels\nlegend = plt.legend()\n\ntitle = plt.gca().set_title(f\"Loss History for the Different Methods\")\n\n\n\n\n\n# Experiment 1\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, 5, 10, 1000)\nfig = plt.plot(LR.score_history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy over time\")\n# as we can see, the accuracy does not converge to 1, but rather oscillates around 0.96\n\n\n\n\n\n# Experiment 2\n# We will now try to change the batch size to compare our loss history\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 2, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 2\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 3, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 3\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 7, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 7\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 15, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 15\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 30, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 30\")\n\nplt.loglog()\n\nlegend = plt.legend() \n# we see that the fastest decrease in loss is for batch size 2, but it also starts at the lowest starting loss value\n\n\n\n\n\n# Experiment 3\n\n# we create data with 10 features\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n# we find the loss history for the stochastic gradient descent with momentum\nLR = LogisticRegression()\nLR.fit_stochastic(X, y,\n                    max_epochs = 100,\n                    momentum = True,\n                    alpha = .05,\n                    batch_size = 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\n# we find the loss history for the stochastic gradient descent without momentum\nLR = LogisticRegression()\nLR.fit_stochastic(X, y,\n                    max_epochs = 100,\n                    momentum = False,\n                    alpha = .05,\n                    batch_size = 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n# we plot it in the log space\nplt.loglog()\n\n# we set the labels\nlegend = plt.legend()\n\n# set the title\ntitle = plt.gca().set_title(f\"Loss History for the Stoachastic Gradient Descent with and without Momentum\")"
  },
  {
    "objectID": "posts/Learning from Timnit Gebru/Learning from Timnit Gebru.html",
    "href": "posts/Learning from Timnit Gebru/Learning from Timnit Gebru.html",
    "title": "Blog Post 8: Learning from Timnit Gebru",
    "section": "",
    "text": "Dr. Gebru discusses the limitations of computer vision in identifying emotions and the potential harms of using facial recognition technology, especially in policing. She argues that we need to consider the implications and potential biases of these technologies before they are implemented, and that we must remember that on both sides of data collection, there are human beings involved. Dr. Gebru then gives an example from her own work called “Gender Shades,” which was released in 2018. The study found a high disparity in error rates among lighter-skinned men and darker-skinned women on APIs with automated facial analysis tools. Specifically, the study analyzed gender classification tools that classify a person in a picture as male or female, and found that the results were not accurate, especially for people with darker skin. Dr. Gebru also discusses the challenges of using race as a category, as it is a social construct that is unstable across time and space. Dr. Gebru and her team had to create their own data set for the Gender Shades study to ensure that it was more balanced by skin type and gender for their analysis.\nA variety of issues with bias and lack of diversity in data sets used in computer vision are then discussed, as well as how this can result in problems such as inaccurate object recognition and biased algorithms. She mentions examples such as how crash tests for cars were based on prototypical male characteristics, resulting in cars that disproportionately harm women and children, and how clinical trials were not tested on women, resulting in drugs that disproportionately harmed them. Dr. Gebru notes that there has been more discussion on fairness, accountability, transparency, and ethics in computer vision recently, but that there is still a lack of understanding on how social structures and problems can affect these issues. Even with a drive to make data-driven systems fairer, such as by having more diverse data sets, there is often not enough consideration of how these systems are actually being used and how they affect the most vulnerable groups. Dr. Gebru argues that it is important to have structural representation and real representation, beyond just diversity of data sets, on ethics boards and panels.Dr. Gebru also discusses how some companies have attempted to address these issues, but have sometimes gone about it in a harmful or non-consensual way, such as scraping images from YouTube without consent.\nIt’s also worrying to hear that flawed facial recognition is being used as evidence in court, which could result in innocent people being wrongly convicted. The issue of automation bias is also important to consider, as it could lead to people blindly following the recommendations of these tools without questioning their accuracy or reliability. It’s clear that more research and action is needed to ensure that facial recognition technology is used in a responsible and ethical manner, and that it doesn’t lead to the infringement of people’s civil liberties or the targeting of certain groups based on their race, ethnicity, or other characteristics.\ntl;dr There are many problems with computer vision and facial recognition technology, particularly in how it perpetuates systemic bias and marginalization of vulnerable groups, so we need engineers and scientists to understand the societal implications of their work, and that they should probably prioritize ethics, fairness, accountability, and transparency in their work, and give frameworks to think about these issues.\nGiven the pervasive use of computer vision technologies to marginalize vulnerable communities, what do you believe are the most urgent steps that engineers and scientists can take to ensure that their work promotes fairness and accountability, rather than perpetuating existing power imbalances?"
  },
  {
    "objectID": "posts/Linear-Regression/Linear-Regression.html",
    "href": "posts/Linear-Regression/Linear-Regression.html",
    "title": "Blog Post 4: Linear Regression",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom linear import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(3)\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train)\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.487\nValidation score = 0.4633\n\n\n\nLR.w\n\narray([0.66678259, 0.94007208])\n\n\n\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train, batch_size= 5, alpha = 0.01, max_iter = 1e2)\nLR2.w\n\narray([0.66766757, 0.94398317])\n\n\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\", title = \"Stochastic Gradient Descent\")\n\n\n\n\n\n# Eperimenting with number of features\ntraining_scores = []\nvalidation_scores = []\n\nfor p_features in range(1, n_train):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR = LinearRegression()\n    LR.fit_gradient(X_train, y_train, batch_size= 3)\n    # if p_features > 35:\n    #     print(p_features, LR.score(X_train, y_train), LR.score(X_val, y_val))\n    training_scores.append(LR.score(X_train, y_train))\n    if p_features > 120:\n        print(\"in between\")\n    validation_scores.append(LR.score(X_val, y_val))\n\nplt.plot(training_scores, label = \"Training\")\nplt.plot(validation_scores, label = \"Validation\")\nplt.legend()\n#  set maximum y value to 1\nplt.ylim(0, 1.1)\nlabels = plt.gca().set(xlabel = \"Number of features\", ylabel = \"Score\", title = \"Linear Regression with Varying Number of Features\")\n\n\n\n\nWe see a sharp increase in training and validation scores as the number of features increase. After the features reach 5, both the scores on average plateau, also the training score is mostly constitently slightly higher than the validation score. As we reach really high number of features, the training validation approaches 100% while the validation slowly starts decreasing which means the model has started overfitting. The validation score would decrease faster, however the stochastic gradient descent slows down the process.\n\nL = Lasso(alpha = 0.001)\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\nL.score(X_val, y_val)\n\n0.8144542416405391\n\n\n\n# Experimenting with number of training examples\ntraining_scores = []\nvalidation_scores = []\n\nfor p_features in range(1, n_train):\n    L = Lasso(alpha = 0.001)\n    X_train, y_train, X_val, y_val = LR_data(p_features, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    training_scores.append(L.score(X_train, y_train))\n    validation_scores.append(L.score(X_val, y_val))\n\nplt.plot(training_scores, label = \"Training\")\nplt.plot(validation_scores, label = \"Validation\")\nplt.legend()\n#  set maximum y value to 1\nplt.ylim(-0.5, 1.1)\nlabels = plt.gca().set(xlabel = \"Number of training examples\", ylabel = \"Score\", title = \"Linear Regression with Varying Number of Training Examples\")\n\n/Users/zayn/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n  model = cd_fast.enet_coordinate_descent(\n/Users/zayn/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n  warnings.warn(msg, UndefinedMetricWarning)\n\n\n\n\n\nWe see the validation results are more volatile than the training results. This is because the validation results are based on a smaller subset of the data, and thus can be more drastically affected by the data points in the validation set."
  }
]