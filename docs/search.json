[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Auditing Allocative Bias.\n\n\n\n\n\n\nMay 23, 2023\n\n\nZayn Makdessi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe ran algorithms on HAR data to determine the best algorithm for HAR\n\n\n\n\n\n\nMay 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nError with mostly Complete Penguins.\n\n\n\n\n\n\nMay 22, 2023\n\n\nZayn Makdessi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression without Bikeshare.\n\n\n\n\n\n\nMay 22, 2023\n\n\nZayn Makdessi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComplete\n\n\n\n\n\n\nApr 19, 2023\n\n\nZayn Makdessi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComplete Gradient Descent.\n\n\n\n\n\n\nMar 1, 2023\n\n\nZayn Makdessi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComplete Perceptron.\n\n\n\n\n\n\nFeb 22, 2023\n\n\nZayn Makdessi\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Zayn",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Perceptron/Perceptron.html",
    "href": "posts/Perceptron/Perceptron.html",
    "title": "Blog Post 1: Perceptron",
    "section": "",
    "text": "When fit is called, the perceptron takes random weights and starts looping until the score reaches 1, or we’ve done max_steps number of loops. We then take the dot product of a random observation with the weight, and if it is negative then we add that abservation to the weight, otherwise if the dot product is positive then we subtract the observation from the weight. After that we check the score of our perceptron using the score function, which in turn calls the predict function and then add the result to our history variable.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nfrom perceptron import Perceptron\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\n# We start by generating a random dataset with 2 classes at opposite ends of the graph which makes it very likely for them to be linearly separable\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n# We then fit the data to the perceptron\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n# We plot the data\nfig = plt.scatter(X[:,0], X[:,1], c = y)\n# Then plot the decision boundary\nfig = draw_line(p.weight[0], -2, 2)\n\n# And label the axes\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy over time\")\n\n\n\n\n\n# We then generate a random dataset with 2 classes that are not linearly separable by having their centers be very close to each other\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(1.0, 0.9), (1.0, 1.0)])\n\n# We then fit the data to the perceptron\np2 = Perceptron()\np2.fit(X2, y2, max_steps = 1000)\n\n# We plot the data\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\n# Then plot the decision boundary\nfig2 = draw_line(p2.weight[0], -2, 2)\n\n# And label the axes\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p2.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy over time\")\n\n\n\n\n\n# We then generate a random dataset with 5 classes that are not linearly separable by having their centers be at the center and the four corners of the graph\nX3, y3 = make_blobs(n_samples = 100, n_features = 5, centers = [(-1.7, -1.7), (1.7, 1.7), (1.7, -1.7), (-1.7, 1.7), (0, 0)])\n# We then fit the data to the perceptron\np3 = Perceptron()\np3.fit(X3, y3, max_steps = 1000)\nfig = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy over time\")\n# seing how the accuracy does not history reach 1 by the end, it is not linearly separable\n\n\n\n\nThe run time of a single iteration of the perceptron is O(p) because the numbers of observations (data points) do not matter, since we are only doing the dot product and then adding p features to p features."
  },
  {
    "objectID": "posts/Penguins/Penguins.html",
    "href": "posts/Penguins/Penguins.html",
    "title": "Blog Post 3: Classifying Palmer Penguins",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Patch\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n# set seed for reproducibility\nnp.random.seed(0)\n\n\nspecies_group = train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Species\"]].groupby('Species').aggregate('mean')\n# display figure\nsns.scatterplot(data=train, x=\"Culmen Length (mm)\", y=\"Body Mass (g)\", hue=\"Species\").set(title=\"Culmen Length vs Body Mass of Different Penguin Species\")\n# display table\nspecies_group\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n    \n    \n      Species\n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      38.710256\n      18.365812\n      189.965812\n      3667.094017\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      48.719643\n      18.442857\n      195.464286\n      3717.857143\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      47.757000\n      15.035000\n      217.650000\n      5119.500000\n    \n  \n\n\n\n\n\n\n\nFirst I only selected a few columns with quantitative data, then grouped the observations by species and saw the mean of the values. Looking at the table and figure we see that Gentoo penguins tend to have a higher body mass, flipper length, and smaller culmen depth compared to the other two species. Meanwhile Adelie penguins tend to have a smaller culmen length and flipper length compared to the other two species. Chinstrap penguins tend to have a similar body mass and culmen depth to Adelie penguins, and similar culmen length to Gentoo penguins. The figure agrees with our observations of the table. We see that Gentoo penguins tend to have a higher body mass, while Adelie penguins tend to have a smaller culmen length. The figure also shows that Chinstrap penguins tend to have a similar body mass to Adelie penguins, and similar culmen length to Gentoo penguins.\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n# I picked the quantitative and qualitative predictors that I thought would be most useful for predicting the species of a penguin.\nall_qual_cols = [\"Clutch Completion\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\"]\n\nbest_cols = 0\nbest_score = 0\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    LR = LogisticRegression(max_iter=3000)\n    LR.fit(X_train[cols], y_train)\n    score = LR.score(X_train[cols], y_train)\n    if score > best_score:\n      best_score = score\n      best_cols = cols\n\nprint(best_cols, best_score)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'] 0.99609375\n\n\nThe predictors that result in the highest accuracy in a logistic regression are Island, Culmen Length, and Culmen Depth. Next we want to see our model’s accuracy on the test set.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n\nLR = LogisticRegression(max_iter=1000)\nLR.fit(X_train[best_cols], y_train)\nLR.score(X_test[best_cols], y_test)\n\n1.0\n\n\nWe found a 100% accuracy on the test set. This is a very high accuracy, and we can be confident that our model is a good predictor of penguin species. Out of Curiosity, I wanted to see if other predictors would be better on different models. I first ran a random forest classifier, decision tree classifier.\n\nbest_cols = 0\nbest_score = 0\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    RF = RandomForestClassifier(max_depth=5)\n    RF.fit(X_train[cols], y_train)\n    score = RF.score(X_train[cols], y_train)\n    if score > best_score:\n      best_score = score\n      best_cols = cols\n\nprint(best_cols, best_score)\nRF = RandomForestClassifier(max_depth=5)\nRF.fit(X_train[best_cols], y_train)\nRF.score(X_test[best_cols], y_test)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'] 1.0\n\n\n0.9852941176470589\n\n\nFor the random forest classifier, we find the same predictors are used, however the testing accuracy is not 100%.\n\nbest_cols = 0\nbest_score = 0\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    DR = DecisionTreeClassifier(max_depth=5)\n    DR.fit(X_train[cols], y_train)\n    score = DR.score(X_train[cols], y_train)\n    if score > best_score:\n      best_score = score\n      best_cols = cols\n\nprint(best_cols, best_score)\nDR = DecisionTreeClassifier(max_depth=5)\nDR.fit(X_train[best_cols], y_train)\nDR.score(X_test[best_cols], y_test)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'] 1.0\n\n\n0.9852941176470589\n\n\nSimilarly for the decision tree classifier, we find the same predictors are used, however the testing accuracy is not 100%.\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1],\n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[best_cols], y_train)\n\n\n\n\nOn Biscoe island, where we only have Gentoo and Adelie penguins, we see a clear decision line because both of these penguin speciees have dissimilar culmen length and depths. On Dream island, where there are Gentoo and Chinstrap penguins, they have a similar distribution of culmen depth, but a bigger difference in culmen length which allows us to place the decision line between them. On Torgersen, there are only Gentoo penguins, so we the decision lines are placed randomly."
  },
  {
    "objectID": "posts/Gradient-Descent/LogisticRegressiono.html",
    "href": "posts/Gradient-Descent/LogisticRegressiono.html",
    "title": "Blog Post 2: Gradient Descent",
    "section": "",
    "text": "LR = LogisticRegression()\nLR.fit_stochastic(X, y, 0.01, 10, 1000)\n\n# inspect the fitted value of w\nLR.w \n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\nloss = LR.empirical_risk(X_, y)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = plt.plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\ntitle = plt.gca().set_title(f\"Loss = {loss}\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\n\n\n# Experiment 1\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, 5, 10, 1000)\n# LR.loss_history\n\n[0.1069168039367764,\n 0.10797678817511525,\n 0.10636153004008612,\n 0.10714108593373343,\n 0.11141679485021486,\n 0.11124219725437028,\n 0.10246056337629167,\n 0.09997356331501515,\n 0.10927368581696129,\n 0.10337479766047863,\n 0.10041772170517062,\n 0.10843191547799265,\n 0.10352614908275964,\n 0.10496464561911388,\n 0.10220367457737847,\n 0.10554084998542404,\n 0.11699660577231004,\n 0.10783245921055759,\n 0.10366066157221016,\n 0.10054258041142732,\n 0.10807811439075184,\n 0.10245210073942818,\n 0.10473880975008058,\n 0.10631899033124716,\n 0.10031082978876732,\n 0.1011866068883192,\n 0.10129510114303382,\n 0.10000484401527167,\n 0.1004234654060928,\n 0.11056507745501967,\n 0.10581340513584092,\n 0.10764097270533583,\n 0.10697292242186421,\n 0.10560538006307246,\n 0.10009586478686289,\n 0.10095361220463783,\n 0.10402500745027311,\n 0.10474368720299186,\n 0.10021873650534989,\n 0.1047154536865298,\n 0.10258926358795488,\n 0.10136379913029334,\n 0.10344911047562587,\n 0.10269653324287052,\n 0.11251472455711702,\n 0.10466059508516423,\n 0.1013632715578644,\n 0.10532221428628304,\n 0.10037861450312226,\n 0.0999428542092721,\n 0.10036306271654424,\n 0.10169818775064482,\n 0.10787295175956527,\n 0.1027781691342378,\n 0.11048922285205,\n 0.10082881872391689,\n 0.10563847140865,\n 0.10065847372255222,\n 0.1042335079413558,\n 0.103387209034269,\n 0.10244510874675107,\n 0.10844425390593879,\n 0.10158322526789315,\n 0.10050978501280305,\n 0.1010390053664975,\n 0.10040448789966465,\n 0.1059467483456504,\n 0.11021098955283719,\n 0.10965004698526522,\n 0.10093509242913758,\n 0.12265871989138899,\n 0.10000514825338773,\n 0.1001517724430593,\n 0.10614440466197216,\n 0.10157243237860403,\n 0.12842343299119421,\n 0.10216971406243676,\n 0.11706965489002999,\n 0.10255375779605243,\n 0.1087613252797543,\n 0.10651596524746615,\n 0.11887998990840522,\n 0.10006200570142136,\n 0.10248269286798237,\n 0.10493067924292646,\n 0.10686197467078111,\n 0.1163193833972236,\n 0.10433955937049173,\n 0.10329771700020697,\n 0.10088142506432282,\n 0.09970428417484016,\n 0.09996477453354867,\n 0.10932965415582059,\n 0.1014829616012513,\n 0.1132983468746654,\n 0.10082937222286777,\n 0.10767716345695687,\n 0.10299630329321605,\n 0.10078257900629777,\n 0.10286002307627484,\n 0.10334275962317115,\n 0.10213573534129884,\n 0.11144957350871602,\n 0.11685970097129057,\n 0.10353773332115715,\n 0.1029737228972072,\n 0.10080695970719432,\n 0.10034693289286249,\n 0.1052698711942967,\n 0.11733402369813285,\n 0.10369940421319353,\n 0.11280175664834509,\n 0.1023689028708181,\n 0.1061346278411021,\n 0.10132535943724029,\n 0.1011569330361002,\n 0.10533399999673225,\n 0.10703604485943309,\n 0.116823913569469,\n 0.10766154334812744,\n 0.1059830129119476,\n 0.10691188076613439,\n 0.1132307930776438,\n 0.10149425480853756,\n 0.10186091743083538,\n 0.10310571757563292,\n 0.10492522520583598,\n 0.10095976407742054,\n 0.10443513929792701,\n 0.11800466277776461,\n 0.1002935240772832,\n 0.10239502888676034,\n 0.1074673333644149,\n 0.10063334753631455,\n 0.1002380799946792,\n 0.12941480477789147,\n 0.10052041969511855,\n 0.10210739938637194,\n 0.10054167801371587,\n 0.10833578070516542,\n 0.10229083992116529,\n 0.10969794932667988,\n 0.10051320742825742,\n 0.10251471429520301,\n 0.10786997099831958,\n 0.1006128499669159,\n 0.10645201933195288,\n 0.10192625251035223,\n 0.10061225191625578,\n 0.10106651896293194,\n 0.10891082510893625,\n 0.10201814203658163,\n 0.10054248125882374,\n 0.10328755541387163,\n 0.10211134450969508,\n 0.10109863103625118,\n 0.10272117812064603,\n 0.10421847562423898,\n 0.10142066330042397,\n 0.10217772474851977,\n 0.1049534663758881,\n 0.10010042627442645,\n 0.10060230595960826,\n 0.10919688989213526,\n 0.10359649101219234,\n 0.10309699184166045,\n 0.10533384614640927,\n 0.10720643595587012,\n 0.1048721710420189,\n 0.10693797987111847,\n 0.11403112090576353,\n 0.10112529198376403,\n 0.1072702552402183,\n 0.1070624841496048,\n 0.100918354351332,\n 0.10074251969393906,\n 0.10817633631726815,\n 0.10389216573324561,\n 0.10019851952493923,\n 0.1011387957326375,\n 0.10142637676299135,\n 0.10015523216973012,\n 0.1063197894885282,\n 0.10075139327477334,\n 0.10264726813278681,\n 0.10267413529534238,\n 0.10196866169383352,\n 0.10130765900479279,\n 0.10421361907927112,\n 0.10835957405852235,\n 0.10101677763828555,\n 0.10283807151999827,\n 0.10547827272688931,\n 0.10053650242029963,\n 0.11040303832223973,\n 0.10401203074863766,\n 0.11335818806096232,\n 0.1017026038567173,\n 0.1005551093024301,\n 0.10415515851985947,\n 0.12080331069068144,\n 0.10982532583735719,\n 0.10198136556609,\n 0.10059002517219548,\n 0.10982677634738348,\n 0.1021648476932247,\n 0.10291743209422809,\n 0.10216314422828064,\n 0.1416775276840668,\n 0.10188201523503655,\n 0.11632089088719529,\n 0.10031874815781379,\n 0.1115229902400428,\n 0.10500313928528669,\n 0.10130431234318085,\n 0.10089811365290291,\n 0.10375434907599086,\n 0.10080229308170786,\n 0.10636736450862017,\n 0.11434912528803651,\n 0.10243402733304498,\n 0.10494910162765278,\n 0.10619229433721683,\n 0.11665209669078201,\n 0.1028319227598979,\n 0.10226619467691514,\n 0.10438684400053055,\n 0.10411310917095806,\n 0.1099399905616296,\n 0.10721044557692312,\n 0.10420625036143795,\n 0.10427187973118374,\n 0.10019486326584466,\n 0.1102852440038439,\n 0.1035241680395384,\n 0.1063567213523023,\n 0.1033007804426266,\n 0.1296597721842153,\n 0.11094816877336239,\n 0.10323185056119963,\n 0.10546184446210972,\n 0.11025045533290961,\n 0.10124025065475097,\n 0.10873864173315004,\n 0.1022288902698559,\n 0.1001433464541907,\n 0.10067881387801213,\n 0.10158717761866969,\n 0.10223181124041897,\n 0.10149717275516482,\n 0.10062722493648452,\n 0.10541337175500097,\n 0.103470994529244,\n 0.10529540751982673,\n 0.10110906483305655,\n 0.10843269425420475,\n 0.1014854372854996,\n 0.10202848971522166,\n 0.10293127168706077,\n 0.10068343342577979,\n 0.1008408979650067,\n 0.1042523983201392,\n 0.10131562882769705,\n 0.10294721190269301,\n 0.10449559565516503,\n 0.1137204643922685,\n 0.12001769473327549,\n 0.10641171615672157,\n 0.10113483907559641,\n 0.10491567255939077,\n 0.10097985234699067,\n 0.12125361279550823,\n 0.1104099458480115,\n 0.10969068513561085,\n 0.10279350962194872,\n 0.1034200188526818,\n 0.10929481732822406,\n 0.10025230605027506,\n 0.10412485147138195,\n 0.10350326337170618,\n 0.10032551532153,\n 0.10167313907471762,\n 0.10217670716978425,\n 0.10081058649128388,\n 0.10149915534255846,\n 0.10109705805287003,\n 0.10068088981490934,\n 0.10094634556595718,\n 0.10918370423505025,\n 0.10085523783228577,\n 0.11615950016206039,\n 0.10459365572603914,\n 0.11021745223775198,\n 0.10659413665232087,\n 0.10426032396503176,\n 0.10283674177790314,\n 0.10982897439402625,\n 0.11250822251099592,\n 0.09991336173495383,\n 0.11189057026128171,\n 0.10030259863557946,\n 0.11270284577194659,\n 0.1021918255499254,\n 0.10013235754802477,\n 0.10963442832378263,\n 0.10525041628484104,\n 0.11329025271560308,\n 0.11065745150729545,\n 0.10708333440729173,\n 0.10476277512004387,\n 0.10154746298986497,\n 0.11443712276370305,\n 0.10042880640377527,\n 0.10062331506986835,\n 0.11631747647628427,\n 0.10075971265280963,\n 0.10310659320080706,\n 0.11609122406279627,\n 0.10031618961118015,\n 0.10238659733408881,\n 0.10073048741599049,\n 0.10289110441790178,\n 0.1120377068492489,\n 0.1048634496495443,\n 0.10298739619209794,\n 0.10329139131559051,\n 0.11739991745790795,\n 0.09986119254983866,\n 0.102979562168226,\n 0.11369812949542148,\n 0.1148180836431833,\n 0.10701256334361166,\n 0.10264734715820759,\n 0.10228634851262072,\n 0.10331800481781699,\n 0.10308148348347829,\n 0.11372903423587456,\n 0.10466188995623205,\n 0.10225342026318597,\n 0.10108172093622222,\n 0.12053158735370709,\n 0.10375525088166292,\n 0.10574317873150116,\n 0.10259515293845585,\n 0.10399936370252227,\n 0.1009271716524324,\n 0.100999535494702,\n 0.11244449586198027,\n 0.10729391258666858,\n 0.11147677638316368,\n 0.11003353450693477,\n 0.10361382696678799,\n 0.10427214604697838,\n 0.10225886112995212,\n 0.12153959693611754,\n 0.10469844222799354,\n 0.10168834126708359,\n 0.10455829833432181,\n 0.10102347033111293,\n 0.11291728867421852,\n 0.11041437026743935,\n 0.10436530673304961,\n 0.10596025950269347,\n 0.10697094579231958,\n 0.10329772512542622,\n 0.10181882203779238,\n 0.101422102490341,\n 0.10471946725398106,\n 0.1064077338796212,\n 0.10039426760501005,\n 0.10203412435550253,\n 0.10370196542705248,\n 0.10026212507586035,\n 0.10176717449891302,\n 0.10327009641540837,\n 0.12204089798314026,\n 0.1254230449874217,\n 0.10844403964170532,\n 0.10084462382390129,\n 0.10775572512794156,\n 0.1045395666060435,\n 0.10786839058298617,\n 0.10139502323456208,\n 0.11422386383221522,\n 0.1057963171096234,\n 0.10270339121733159,\n 0.10150598767123901,\n 0.10209578072085676,\n 0.10021544020632284,\n 0.10348748310959945,\n 0.1089362874840122,\n 0.10138505593618621,\n 0.10446832927087485,\n 0.10409479741457027,\n 0.10095237694884876,\n 0.10022791023155747,\n 0.1093660521641307,\n 0.1048737445802294,\n 0.10771307712296288,\n 0.10194016449829636,\n 0.10049145945436459,\n 0.10130713963136528,\n 0.1089125376169692,\n 0.11348293119402811,\n 0.10327769670109728,\n 0.10384786738870527,\n 0.10116666339811672,\n 0.100168193661761,\n 0.10145018077254642,\n 0.11675190945417598,\n 0.10330674053814687,\n 0.10038452519008674,\n 0.11202642480728517,\n 0.10484025380071887,\n 0.1225794935047469,\n 0.10807944291221414,\n 0.1001920670967931,\n 0.10565870727013711,\n 0.10933853512121917,\n 0.11674745615572561,\n 0.10077648493504444,\n 0.10241042371757729,\n 0.11096315567421108,\n 0.1044203221847238,\n 0.10642696797237132,\n 0.108746703015455,\n 0.1015525680498497,\n 0.10137360361784775,\n 0.10184196842685153,\n 0.10288301120920598,\n 0.10507748707248445,\n 0.10112475169248342,\n 0.10237837865646993,\n 0.1034893005759464,\n 0.10756273586118038,\n 0.10000044849201598,\n 0.10805564418228983,\n 0.10332619515877915,\n 0.10071066704109534,\n 0.10296340263209607,\n 0.1037416245346793,\n 0.11545341447992451,\n 0.10058278574606355,\n 0.10686291131831042,\n 0.10054595548596104,\n 0.12279585061352374,\n 0.10231680245245905,\n 0.10028743873618912,\n 0.09985442116173654,\n 0.10247215044963091,\n 0.10516032443608035,\n 0.10962449536190816,\n 0.1111816296639122,\n 0.10882098376508534,\n 0.11459293113561579,\n 0.10155299274998442,\n 0.10919107445361,\n 0.10530394477177543,\n 0.10236774028083774,\n 0.11470336066921889,\n 0.10354480945152661,\n 0.1205709185935051,\n 0.11482999197574935,\n 0.10381389787717168,\n 0.1055363803506216,\n 0.10797199395777662,\n 0.11653723305838325,\n 0.10620809782728294,\n 0.10339973522530693,\n 0.1092486796599753,\n 0.10919420482135081,\n 0.10895764582031131,\n 0.10679745147139426,\n 0.10013875751007621,\n 0.1252433241462832,\n 0.1009957110441662,\n 0.10410128418366454,\n 0.10773915475596939,\n 0.10745786035045211,\n 0.10873309794995265,\n 0.10267982218211731,\n 0.10996308146534937,\n 0.10250904533664446,\n 0.10511437363490991,\n 0.10337575658737652,\n 0.10341986280055117,\n 0.10392610771825225,\n 0.10036215705997208,\n 0.1018309580094223,\n 0.10087749036196714,\n 0.10387549594706474,\n 0.10186837243002547,\n 0.09967793894616797,\n 0.10041888982370556,\n 0.09981492446186241,\n 0.09993107562395127,\n 0.10371976703077185,\n 0.10015451531934282,\n 0.10136263376435073,\n 0.10038402593049044,\n 0.10353456436661983,\n 0.10315519439959019,\n 0.10174918598593333,\n 0.11007678264527548,\n 0.12970117148206975,\n 0.10126014174036811,\n 0.12445445267939799,\n 0.10556671731918929,\n 0.10247481452880829,\n 0.10833334649587628,\n 0.10146753450909728,\n 0.1023297553265709,\n 0.10169668511351432,\n 0.10502136172472133,\n 0.10341231691824754,\n 0.10881579961622803,\n 0.09994520549932959,\n 0.11259902887374208,\n 0.10667698492938774,\n 0.10780768867507576,\n 0.1034014359111953,\n 0.11223104100328751,\n 0.10731817996467104,\n 0.10077236443042917,\n 0.10313063340380228,\n 0.10566880826552834,\n 0.1020029383339488,\n 0.10484621843727952,\n 0.10865659910500888,\n 0.10027469064466878,\n 0.10626070369242395,\n 0.10201743576470776,\n 0.1024792239206773,\n 0.10012671578591188,\n 0.10983540539363477,\n 0.10142754473254857,\n 0.12055335705309575,\n 0.10006008987646578,\n 0.11430898121475642,\n 0.10514435737953842,\n 0.1012167107205873,\n 0.10384813525038165,\n 0.1016656468119994,\n 0.10023534832828605,\n 0.10695232356246458,\n 0.10530870941864207,\n 0.10445771349166195,\n 0.11092323949664394,\n 0.10002911960897949,\n 0.10047536680822107,\n 0.10122146704962566,\n 0.10231186650551033,\n 0.10391932735373498,\n 0.1123611084191112,\n 0.10294143061614172,\n 0.10101968606162426,\n 0.10093617925016103,\n 0.12773629042725176,\n 0.10578904045212704,\n 0.10068236926346025,\n 0.11169617311031858,\n 0.10622939295106637,\n 0.10028328662831357,\n 0.10107909122851161,\n 0.10707577883114716,\n 0.10276476791136763,\n 0.10026483713293144,\n 0.11127166943918337,\n 0.11118945133543548,\n 0.10177349823894685,\n 0.10626931151463785,\n 0.10360075072885239,\n 0.10582103580061243,\n 0.10253727866049096,\n 0.10103896563587686,\n 0.10191136215508337,\n 0.10056825611452677,\n 0.10157439602896132,\n 0.10047474707424545,\n 0.10055239825488865,\n 0.1046134426331957,\n 0.11481650769972877,\n 0.10069350095354218,\n 0.10033747987563515,\n 0.11124945253463762,\n 0.10353291716683088,\n 0.10238486149607151,\n 0.11987447996638696,\n 0.10315246073535844,\n 0.11118029528489248,\n 0.1007871048755612,\n 0.10053564597443966,\n 0.1018788016838116,\n 0.10059847045409569,\n 0.10323776476825561,\n 0.10008770559479054,\n 0.10390506486603979,\n 0.10615759913955347,\n 0.10182189054124667,\n 0.11384961979957342,\n 0.10014714970193472,\n 0.11209532924572654,\n 0.10229913626580918,\n 0.10558447288294831,\n 0.10573232800169591,\n 0.11221674781542713,\n 0.10095088765010388,\n 0.1008424220061986,\n 0.11824351577963084,\n 0.10405473154626219,\n 0.10590794883065346,\n 0.12078299545014026,\n 0.11336385224379124,\n 0.10402436484861194,\n 0.10254309177192202,\n 0.10691216779753625,\n 0.10042986038739223,\n 0.10133818672232614,\n 0.10224704640790402,\n 0.10040247462874102,\n 0.09974491930827693,\n 0.10579933050215642,\n 0.10563537413965818,\n 0.10656108215919881,\n 0.12185272343988199,\n 0.10975157065538026,\n 0.10341866160394293,\n 0.11313993832733267,\n 0.10529347218964352,\n 0.10705940376098369,\n 0.11115043275941465,\n 0.10435882038213393,\n 0.10058463915312119,\n 0.10207683399828166,\n 0.10500115589087862,\n 0.10581947745265738,\n 0.1037362053675585,\n 0.10646192858389014,\n 0.10083727256934319,\n 0.10538471412016437,\n 0.10625361778001499,\n 0.10237802643981041,\n 0.10259079319148733,\n 0.10502581900924396,\n 0.10054447647357753,\n 0.10050622516556373,\n 0.10761516757296487,\n 0.10368140918477696,\n 0.10131440444560902,\n 0.10805563038892622,\n 0.1002430573528915,\n 0.10242861469129981,\n 0.10311765753720184,\n 0.10484116787715028,\n 0.1082304577383238,\n 0.10285727114153861,\n 0.11266913195343868,\n 0.10984525001644263,\n 0.10085446643616529,\n 0.10074814521250464,\n 0.10097161725135298,\n 0.1012106963672709,\n 0.10589536374767572,\n 0.10252137354711272,\n 0.10098173950284436,\n 0.10044650787714388,\n 0.10463410159485848,\n 0.10007732582006543,\n 0.10785985621977044,\n 0.09976481367013744,\n 0.10192880638850088,\n 0.10492291590880125,\n 0.10022110577235561,\n 0.10680778791667546,\n 0.11740973558576943,\n 0.11165754261354956,\n 0.10487956372158504,\n 0.10241006623368572,\n 0.10019513281856866,\n 0.10848845691065874,\n 0.11151025596818924,\n 0.10145304491622895,\n 0.1092855029292295,\n 0.10252510041019391,\n 0.11376918431328827,\n 0.10388175234616469,\n 0.10639936462642914,\n 0.10151333724333558,\n 0.10093468063901426,\n 0.10043688984980434,\n 0.10055200708618284,\n 0.1278034520057391,\n 0.10910219792094605,\n 0.10136029358172938,\n 0.10740992629729568,\n 0.10223922061033544,\n 0.1149267974846875,\n 0.10049126695398117,\n 0.11374759692984152,\n 0.11293743649855444,\n 0.1137132413239178,\n 0.10290241747179846,\n 0.1055303466662769,\n 0.11356098413893903,\n 0.10955784170425922,\n 0.10207683023409504,\n 0.10417000791022034,\n 0.11588389932924842,\n 0.10008841959121277,\n 0.1041816913507644,\n 0.10519660808291328,\n 0.10587025531896505,\n 0.10035328081223845,\n 0.10794972287019426,\n 0.10785469948661848,\n 0.10331397509944683,\n 0.10066926879545975,\n 0.10741686314646193,\n 0.10439783613727506,\n 0.11154406262372561,\n 0.10686895514662396,\n 0.10812072585537522,\n 0.10058916956885287,\n 0.1030530008017255,\n 0.10004956098504043,\n 0.10258833524566739,\n 0.10390994705956055,\n 0.10758336397655213,\n 0.1043361106176377,\n 0.10630890124808828,\n 0.10066402802431265,\n 0.1030384577706813,\n 0.10258239576696077,\n 0.10314240135184129,\n 0.10520973692095162,\n 0.12337271019605915,\n 0.1132782436153314,\n 0.10025883502896217,\n 0.10092666307686389,\n 0.10142877138861543,\n 0.11010731017839767,\n 0.10453889967973823,\n 0.1006180046399374,\n 0.1049025036371425,\n 0.10180169336330365,\n 0.10065155544353889,\n 0.10742993452809646,\n 0.11444750722737945,\n 0.09995300833022068,\n 0.10039414016148082,\n 0.1145321311781349,\n 0.10555949629843328,\n 0.09985119130819706,\n 0.1044561455724319,\n 0.10426933810501932,\n 0.10378117300141529,\n 0.11137690359734838,\n 0.10063978175768075,\n 0.10099648585973559,\n 0.1125465241014048,\n 0.10026629910413666,\n 0.10089511381342195,\n 0.10071255376704384,\n 0.10504633823171393,\n 0.11274259796056114,\n 0.10189047226647753,\n 0.10410190361697641,\n 0.10478942924380855,\n 0.1152316611257184,\n 0.10192961627501104,\n 0.1009984957444643,\n 0.10293389571440835,\n 0.10295052260319308,\n 0.10108797192800381,\n 0.10095434137592708,\n 0.1023686971053823,\n 0.10740070325564503,\n 0.10230672522858178,\n 0.1028776396106269,\n 0.10146338464721869,\n 0.10738480342742754,\n 0.10721096766556311,\n 0.11154037360879972,\n 0.11112902119237388,\n 0.10302215013066786,\n 0.10007016685912681,\n 0.1166029806650465,\n 0.11219200252515435,\n 0.10001083565291313,\n 0.14404052601412481,\n 0.12057465597541686,\n 0.10215275390731232,\n 0.11087657356287468,\n 0.10021687621192012,\n 0.09981894031231171,\n 0.10585404829964698,\n 0.10236124388960119,\n 0.10019261145892575,\n 0.10923008468345184,\n 0.10013929310943834,\n 0.11031627838564743,\n 0.1150975253620109,\n 0.10441043366630776,\n 0.10717779657897884,\n 0.11412066859009144,\n 0.10032863773606959,\n 0.10913736915662248,\n 0.10397742190890095,\n 0.11025718664027134,\n 0.10818353236162252,\n 0.10816697448121924,\n 0.10038532485762872,\n 0.1055141962679419,\n 0.10045541932508575,\n 0.10186167693369484,\n 0.10937844993326248,\n 0.1124891019378055,\n 0.1011600825356282,\n 0.10227771748719419,\n 0.10438219367877846,\n 0.10808798446068917,\n 0.10278859460815554,\n 0.11663062273584053,\n 0.10693443185697905,\n 0.1038388378754988,\n 0.10095065600358188,\n 0.10445481904293588,\n 0.10225641598064833,\n 0.1080564296452544,\n 0.10152739956851359,\n 0.10280659291893404,\n 0.10405542800779337,\n 0.10437859567707534,\n 0.10543477563399202,\n 0.10697588372714799,\n 0.10683972363695494,\n 0.12572666407655597,\n 0.10441448684455568,\n 0.1061915826080472,\n 0.10052139485061072,\n 0.10134298752354802,\n 0.11122932035831522,\n 0.10205189166199852,\n 0.12171637629008679,\n 0.10446149567469132,\n 0.10071642107194972,\n 0.11150827644867721,\n 0.10633963089543755,\n 0.10160588985915248,\n 0.10943561891621414,\n 0.11323672676420289,\n 0.10023506363139041,\n 0.10502163150904018,\n 0.10469864133937837,\n 0.10306712871934748,\n 0.11401687354941746,\n 0.10219614617276934,\n 0.10389862565403693,\n 0.10726527835864275,\n 0.12150744558367585,\n 0.10163341642278305,\n 0.11195068310162984,\n 0.10351154705787127,\n 0.10378819187283173,\n 0.10062219939318619,\n 0.10117773950561947,\n 0.1112516270459689,\n 0.10783917615743226,\n 0.10801434440889796,\n 0.10154673544042915,\n 0.1078211032429064,\n 0.10291401975352958,\n 0.10294626101360875,\n 0.1062479702678887,\n 0.11177648278504665,\n 0.1022099427502166,\n 0.1090234274204437,\n 0.10466233247913304,\n 0.10436886422515163,\n 0.1068996199448884,\n 0.108573304791531,\n 0.1006427917418743,\n 0.10606876237433109,\n 0.10300888020621986,\n 0.10482131899792194,\n 0.10061334769670761,\n 0.10501428878523118,\n 0.11760277252822608,\n 0.10427377234452045,\n 0.09990940477376035,\n 0.10071355799322156,\n 0.108951931925628,\n 0.10299372524178027,\n 0.11289194465449034,\n 0.10018012949191071,\n 0.10184892744658779,\n 0.10435891405901854,\n 0.10002154397894064,\n 0.10579866518803087,\n 0.10153360497816614,\n 0.101408761992264,\n 0.10062547689803601,\n 0.10201050988684283,\n 0.10856142133693222,\n 0.10443221512216123,\n 0.10060861976857051,\n 0.11578671243603321,\n 0.10044632018636483,\n 0.11165057937975345,\n 0.10644118716116051,\n 0.10140000300773779,\n 0.10130205907882484,\n 0.10129384792469093,\n 0.1027055871659718,\n 0.09993987907052528,\n 0.10184773614163838,\n 0.10264218800283466,\n 0.10779138000360679,\n 0.10010225622679865,\n 0.10462989050338166,\n 0.10060650991648261,\n 0.1049115499951229,\n 0.10801736033826188,\n 0.10173686208366281,\n 0.10917958177396671,\n 0.10135288006817053,\n 0.11148194427138852,\n 0.12296245748898188,\n 0.10135188112141684,\n 0.10211894225537364,\n 0.11885585570350511,\n 0.10129908185762954,\n 0.10255875013524811,\n 0.10490084310532516,\n 0.10708145009480913,\n 0.10228859788576006,\n 0.10312981191030669,\n 0.10169805747569066,\n 0.10480590370219449,\n 0.1025707127270491,\n 0.10439256253598345,\n 0.1016878830389874,\n 0.11182432780021735,\n 0.11158087045331787,\n 0.10409639632687895,\n 0.10879905192254165,\n 0.10298093200091214,\n 0.10905865918966477,\n 0.10939623347618177,\n 0.1030613526019712,\n 0.10411912533760902,\n 0.10227350625427273,\n 0.10021144707325583,\n 0.10476458667622915,\n 0.10346589099481801,\n 0.11329194131144821,\n 0.10091014760302905,\n 0.11666001794788027,\n 0.10203140515527012,\n 0.11209780932906854,\n 0.1013751334266637,\n 0.1164578114600625,\n 0.10288881808387135,\n 0.10692063678082044,\n 0.1003736894000925,\n 0.10107616463270419,\n 0.10071373208167884,\n 0.10487419245246409,\n 0.10438626855624811,\n 0.11139115269441642,\n 0.10333934308875042,\n 0.10277032114156494,\n 0.10704177764273667,\n 0.10537101664017812,\n 0.10015853066986427,\n 0.10470121563703337,\n 0.11188761165407078,\n 0.1050022411662345,\n 0.10137662049634544,\n 0.10138372706374459,\n 0.1027226319891945,\n 0.10146079027453046,\n 0.10402815098464441,\n 0.10503696881121531,\n 0.10574039304494445,\n 0.11476564136207135,\n 0.10334249947393749,\n 0.10706824961449925,\n 0.10771277869676847,\n 0.10102433741090186,\n 0.10000532873620063,\n 0.10813953546761151,\n 0.10180981558899305,\n 0.10234150776316998,\n 0.10205660837110765,\n 0.09984037630525823,\n 0.10327184279756539,\n 0.11083424280671032,\n 0.10006830501957972,\n 0.11100230575827105]\n\n\n\n# Experiment 2\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 2, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 2\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 3, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 3\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 7, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 7\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 15, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 15\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 30, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 30\")\n\nplt.loglog()\n\nlegend = plt.legend()"
  },
  {
    "objectID": "posts/Gradient-Descent/LogisticRegression.html",
    "href": "posts/Gradient-Descent/LogisticRegression.html",
    "title": "Blog Post 2: Gradient Descent",
    "section": "",
    "text": "Introduction\nIn this blog post I will be demonstrating my implementation of the Gradient Descent Algorithm and show it in action for the logistic regression problem. I will also be comparing it to the Stochastic Gradient Descent Algorithm and show how it can be used to improve the performance of the Gradient Descent Algorithm. I will also be showing how the momentum feature can be used to improve the performance of the Stochastic Gradient Descent Algorithm. Then, I will perform some experiments on synthetic data to show how the learning rate, batch size and momentum affect the performance of the Stochastic Gradient Descent Algorithm.\n\nfrom logistic_regression import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\nnp.random.seed(1234)\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, 0.01, 10, 1000)\n\n# inspect the fitted value of w\nLR.w \n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n# calculate the loss\nloss = LR.empirical_risk(X_, y)\n\n# plot the decision boundary\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\n# we plot the data with the decision boundary\np = plt.plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\ntitle = plt.gca().set_title(f\"Loss = {loss}\")\n\n\n\n\n\n# we find the loss history for the stochastic gradient descent with momentum\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  alpha = .05,\n                  batch_size = 10) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\n# we find the loss history for the stochastic gradient descent without momentum\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 10, \n                  alpha = .5)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n# we find the loss history for the gradient descent\nLR = LogisticRegression()\nLR.fit(X, y,\n       alpha = .05,\n       max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\n# we set it in the log space\nplt.loglog()\n\n# we set the labels\nlegend = plt.legend()\n\ntitle = plt.gca().set_title(f\"Loss History for the Different Methods\")\n\n\n\n\n\n\nExperiment 1\nI will run a stochastic gradient descent with a large alpha.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, 5, 10, 1000)\nfig = plt.plot(LR.score_history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy over time\")\n\n\n\n\nAs we can see, the accuracy does not converge to 1, but rather oscillates around 0.96. This is because the learning rate is too large and the algorithm overshoots the minimum.\n\n\nExperiment 2\nI will now change the batch size to compare our loss history.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 2, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 2\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 3, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 3\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 7, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 7\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 15, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 15\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  batch_size = 30, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Batch size 30\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nWe see that the fastest decrease in loss is for batch size 2, but it also starts at the lowest starting loss value.\n\n\nExperiment 3\nI will now compare stochastic gradient descent with and without momentum’s loss history.\n\n# we create data with 10 features\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n# we find the loss history for the stochastic gradient descent with momentum\nLR = LogisticRegression()\nLR.fit_stochastic(X, y,\n                    max_epochs = 100,\n                    momentum = True,\n                    alpha = .05,\n                    batch_size = 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\n# we find the loss history for the stochastic gradient descent without momentum\nLR = LogisticRegression()\nLR.fit_stochastic(X, y,\n                    max_epochs = 100,\n                    momentum = False,\n                    alpha = .05,\n                    batch_size = 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n# we plot it in the log space\nplt.loglog()\n\n# we set the labels\nlegend = plt.legend()\n\n# set the title\ntitle = plt.gca().set_title(f\"Loss History for the Stochastic Gradient Descent with and without Momentum\")\n\n\n\n\nWe see that stochastic gradient descent with momentum has a much faster decrease in loss than stochastic gradient descent without momentum.\n\n\nConclusion\nI have compared the three algorithms and shown how they can be used to solve the logistic regression problem. I have also shown how the learning rate, batch size and momentum affect the performance of the stochastic gradient descent algorithm."
  },
  {
    "objectID": "posts/Learning from Timnit Gebru/Learning from Timnit Gebru.html",
    "href": "posts/Learning from Timnit Gebru/Learning from Timnit Gebru.html",
    "title": "Blog Post 8: Learning from Timnit Gebru",
    "section": "",
    "text": "Part 2\nIn her discussions, both in our class and during her public talk, Dr. Timnit Gebru delved into the complex issues surrounding artificial intelligence. While AI was the overarching topic, she placed a significant focus on the individuals and entities who wield control and gain influence as a result. Dr. Gebru meticulously traced the historical progression of the eugenics movement, revealing how it has been widely accepted by powerful individuals throughout time. She drew compelling connections between contemporary movements, such as effective altruism, and earlier eugenics movements that aimed to eliminate perceived negative traits from the human race. By demonstrating the lineage of these ideas, she alleged that the current movements are eugenicist and she condemend the influence they exert on influential figures within the tech industry and investment sphere.\nMoreover, Dr. Gebru explored how the pursuit of Artificial General Intelligence (AGI) has influenced the development of AI systems. She highlighted the inherent harm caused by the pursuit of AGI, particularly for vulnerable communities. The concept of AGI, with its aspiration for a single all-powerful machine, naturally leads to the centralization of power. This raises concerns regarding those who hold authority over the design and creation of such systems. Dr. Gebru argued that many of these individuals already possess flawed views about the future trajectory of humanity, further emphasizing the potential negative consequences of AGI.\nAnother crucial aspect of Dr. Gebru’s talks revolved around the flawed perception of AI in the media. She elaborated how the lack of comprehensive knowledge about AI functioning and the grand promises made by prominent companies significantly impact the funding landscape. As a result, investors often hesitate to support smaller organizations that are genuinely working on meaningful advancements in AI. This skewed preference for established corporations, even when they deliver subpar or non-existent products, perpetuates a cycle that stifles innovation and undermines the potential positive impact of AI development.\nI agree with Dr. Gebru’s points since I also find it essential for people to become aware of the greed of influential figures within the tech industry who desire to further enrich thethemselves, rather than helping the broader population. Furthermore, it is crucial to acknowledge the significant influence these individuals wield over the future of technology. Hopefully, by doing so, a more nuanced understanding can be gained, highlighting the inherent risks and potential inequalities that may arise from the creation of powerful AI systems. I also believe that her arguments against AGI extend beyond the realm of artificial intelligence and can be applied to critique various other technological advancements.\nI found myself thinking a lot more about the ethics and potential harm/ personal gain in developping technologies, and will continue to do so moving forward. It does sadden me how big companies have the ability to develop technologies to help people but instead choose not to as it does not benefit those at the top. I will keep this in mind when I am working in the future and will try to make sure that I am not contributing to the problem. I will try to only work for companies who’s values align with mine, and attempt to use my skills to help impact the trajectory of developping technology."
  },
  {
    "objectID": "posts/Linear-Regression/Linear-Regression.html",
    "href": "posts/Linear-Regression/Linear-Regression.html",
    "title": "Blog Post 4: Linear Regression",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom linear import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(3)\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train)\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.487\nValidation score = 0.4633\n\n\n\nLR.w\n\narray([0.66678259, 0.94007208])\n\n\n\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train, batch_size= 5, alpha = 0.01, max_iter = 1e2)\nLR2.w\n\narray([0.66766757, 0.94398317])\n\n\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\", title = \"Stochastic Gradient Descent\")\n\n\n\n\n\n# Eperimenting with number of features\ntraining_scores = []\nvalidation_scores = []\n\nfor p_features in range(1, n_train):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR = LinearRegression()\n    LR.fit_gradient(X_train, y_train, batch_size= 3)\n    # if p_features > 35:\n    #     print(p_features, LR.score(X_train, y_train), LR.score(X_val, y_val))\n    training_scores.append(LR.score(X_train, y_train))\n    if p_features > 120:\n        print(\"in between\")\n    validation_scores.append(LR.score(X_val, y_val))\n\nplt.plot(training_scores, label = \"Training\")\nplt.plot(validation_scores, label = \"Validation\")\nplt.legend()\n#  set maximum y value to 1\nplt.ylim(0, 1.1)\nlabels = plt.gca().set(xlabel = \"Number of features\", ylabel = \"Score\", title = \"Linear Regression with Varying Number of Features\")\n\n\n\n\nWe see a sharp increase in training and validation scores as the number of features increase. After the features reach 5, both the scores on average plateau, also the training score is mostly constitently slightly higher than the validation score. As we reach really high number of features, the training validation approaches 100% while the validation slowly starts decreasing which means the model has started overfitting. The validation score would decrease faster, however the stochastic gradient descent slows down the process.\n\nL = Lasso(alpha = 0.001)\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\nL.score(X_val, y_val)\n\n0.8144542416405391\n\n\n\n# Experimenting with number of training examples\ntraining_scores = []\nvalidation_scores = []\n\nfor p_features in range(1, n_train):\n    L = Lasso(alpha = 0.001)\n    X_train, y_train, X_val, y_val = LR_data(p_features, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    training_scores.append(L.score(X_train, y_train))\n    validation_scores.append(L.score(X_val, y_val))\n\nplt.plot(training_scores, label = \"Training\")\nplt.plot(validation_scores, label = \"Validation\")\nplt.legend()\n#  set maximum y value to 1\nplt.ylim(-0.5, 1.1)\nlabels = plt.gca().set(xlabel = \"Number of training examples\", ylabel = \"Score\", title = \"Linear Regression with Varying Number of Training Examples\")\n\n/Users/zayn/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n  model = cd_fast.enet_coordinate_descent(\n/Users/zayn/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n  warnings.warn(msg, UndefinedMetricWarning)\n\n\n\n\n\nWe see the validation results are more volatile than the training results. This is because the validation results are based on a smaller subset of the data, and thus can be more drastically affected by the data points in the validation set."
  },
  {
    "objectID": "posts/ML-Project/project-post.html",
    "href": "posts/ML-Project/project-post.html",
    "title": "Human Activity Recognition (HAR) - Final Project",
    "section": "",
    "text": "We worked with the Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set from the University of California, Irvine. We first did some preliminary analysis on the data and that the static activities have stagnant acceleration and gyroscope while the movement activities has significantly more activity. We then implemented three algorithms, the K-nearest neighbors algorithm, the multilayer perceptron (also known as a fully connected neural network), and a random forest classifier and compared their accuracy on cross-validation. We found that the K-nearest neighbors gave us the best accuracy with a single neighbor.\nLink to Github Repository"
  },
  {
    "objectID": "posts/ML-Project/project-post.html#values-statement",
    "href": "posts/ML-Project/project-post.html#values-statement",
    "title": "Human Activity Recognition (HAR) - Final Project",
    "section": "Values Statement",
    "text": "Values Statement\nOur algorithm helps identify a task that someone is doing. It would be used in a smart watch or on our phone to track our health. How often do you stand up during the day? How much do you walk a day? It could provide useful information to create a better lifestyle. For example, within an app, the algorithm would track how much someone walks during the day. Assuming that person checks that app, they may notice that they don’t walk around as much as they should and would influence them into exercising more. It would create a better lifestyle for that person, which would help them in the long run.\nThere could be some harm done if the device using the algorithm is not accurate. For example, if the algorithm is not accurate, it could tell someone that they are walking when they are actually sitting down. This could lead to someone thinking that they are getting more exercise than they actually are. This could lead to someone not exercising as much as they should. This could lead to health problems in the future. Outside of health, another potential harm is if the algorithm is fed biased data. For example, if the device is used to determine if someone is walking or running, it could be biased against people with physical disabilities.\nI find Human Activity Recognition to be an extremely interesting topic and direction for technology to evolve in. I believe that it is important to understand how our bodies move and how we can use that information to improve our lives.\nFor this reason, I believe that implementing this will help people achieve a better lifestyle, which makes the world a more joyful place."
  },
  {
    "objectID": "posts/Allocative-Bias/bias.html",
    "href": "posts/Allocative-Bias/bias.html",
    "title": "Blog Post 5: Auditing Allocative Bias",
    "section": "",
    "text": "from folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\n\nnp.random.seed(0)\n\n\nSTATE = \"MA\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      77\n      19.0\n      3\n      16\n      2\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n    \n    \n      1\n      18\n      18.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      2.0\n      2\n      9\n      1.0\n    \n    \n      2\n      28\n      21.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      2.0\n      1\n      1\n      1.0\n    \n    \n      3\n      22\n      19.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      1\n      1\n      6.0\n    \n    \n      4\n      50\n      1.0\n      5\n      17\n      1\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      1\n      1.0\n      2\n      1\n      6.0\n    \n  \n\n\n\n\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\nmodel = make_pipeline(StandardScaler(), LogisticRegression())\nmodel.fit(X_train, y_train)\ny_hat = model.predict(X_test)\ntot_acc = (y_hat == y_test).mean()\nwhite_acc = (y_hat == y_test)[group_test == 1].mean()\nblack_acc = (y_hat == y_test)[group_test == 2].mean()\nprint(f\"Total accuracy: {tot_acc}\")\nprint(f\"The accuracy for white individuals: {white_acc}\")\nprint(f\"The accuracy for black individuals: {black_acc}\")\n\nTotal accuracy: 0.7803521779425394\nThe accuracy for white individuals: 0.7833114897335081\nThe accuracy for black individuals: 0.7806122448979592\n\n\n\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\nprint(f\"Total number of individuals: {len(df)}\")\nprint(f\"Proportion of individuals with target label equal to 1: {df['label'].mean()}\")\nprint(f\"Number of individuals in each group: {df.groupby('group')['label'].count()}\")\nprint(f\"Proportion of individuals with target label equal to 1 in each group: {df.groupby('group')['label'].mean()}\")\n\n# only looking at white, black, and asian individuals because of small sample size of other races\nrace_list = [1, 2, 6]\nfiltered = df.query(\"group in @race_list\")\n\n# group by race and sex then check employment\ngrouped = filtered.groupby(['group', 'SEX'])['label'].mean().reset_index()\n\nraces = [\"White\", \"Black\", \"Asian\",]\n\nplot = sns.barplot(data= grouped, x='group', y='label', hue='SEX')\nplot.set(title=\"Average Employment by Race and Sex\", xlabel=\"Race\", ylabel=\"Average Employment\")\nplot.set_xticklabels(races)\n\nTotal number of individuals: 56104\nProportion of individuals with target label equal to 1: 0.5056858691002424\nNumber of individuals in each group: group\n1    45515\n2     3405\n3       66\n4        1\n5       24\n6     3778\n7       24\n8     1698\n9     1593\nName: label, dtype: int64\nProportion of individuals with target label equal to 1 in each group: group\n1    0.514556\n2    0.456681\n3    0.500000\n4    1.000000\n5    0.500000\n6    0.510058\n7    0.625000\n8    0.469376\n9    0.383553\nName: label, dtype: float64\n\n\n[Text(0, 0, 'White'), Text(1, 0, 'Black'), Text(2, 0, 'Asian')]\n\n\n\n\n\nWe see that white, black, and asian women all have about the same average employment rate of 49%. Meanwhile white and asian men have an average employment rate of 54%, while black men have the lowest average employment rate among these 6 groups at 43%.\n\naccuracies = []\n\nfor i in range(2, 20):\n    RF = RandomForestClassifier(n_estimators=100, max_depth=i, random_state=0)\n    RF.fit(X_train, y_train)\n    y_hat = RF.predict(X_test)\n    accuracies.append((y_hat == y_test).mean())\n\nplt.plot(range(2, 20), accuracies)\n\n\n\n\nLooking at the plot, it seems like the best choice for max depth is 14, because after that the accuracy does start to decrease. However it is only slightly more accurate than a max depth of 5, but will take longer to run.\n\n# Recaculate with max_depth = 16\nRF = RandomForestClassifier(n_estimators=100, max_depth=16, random_state=0)\nRF.fit(X_train, y_train)\ny_hat = RF.predict(X_test)\n\n\nprint(f\"Total accuracy: {(y_hat == y_test).mean()}\")\n\n# positive predictive value (PPV) of your model\n(tn, fp), (fn, tp) = confusion_matrix(y_test, y_hat)\nppv = tp / (tp + fp)\nprint(f\"Positive predictive value: {ppv}\")\n\n# false negative rate (FNR) of your model\nfnr = fn / (fn + tp)\nprint(f\"False negative rate: {fnr}\")\n\n# false positive rate (FPR) of your model\nfpr = fp / (fp + tn)\nprint(f\"False positive rate: {fpr}\")\n\n# in a table\nraces = [\"White\", \"Black\", \"American Indian\", \"American Indian and Alaska Native\", \"Asian\", \"Native Hawaiian\"]\ncolumns = [\"Accuracy\", \"PPV\", \"FNR\", \"FPR\", \"Prediction Rate\"]\ndatf = pd.DataFrame(columns=columns, index=races)\nresults = [[], [], [], [], []]\n\n# accuracy of your model on each subgroup\n[results[0].append((y_hat == y_test)[group_test == i].mean()) for i in range(1, 8) if i != 4]\n\n# PPV of your model on each subgroup\nfor i in range(1, 8):\n    if i != 4:\n        tn, fp, fn, tp = confusion_matrix(y_test[group_test == i], y_hat[group_test == i]).ravel()\n        ppv = tp / (tp + fp)\n        results[1].append(ppv)\n\n\n# FNR of your model on each subgroup\nfor i in range(1, 8):\n    if i != 4:\n        tn, fp, fn, tp = confusion_matrix(y_test[group_test == i], y_hat[group_test == i]).ravel()\n        fnr = fn / (fn + tp)\n        results[2].append(fnr)\n\n# FPR of your model on each subgroup\nfor i in range(1, 8):\n    if i != 4:\n        tn, fp, fn, tp = confusion_matrix(y_test[group_test == i], y_hat[group_test == i]).ravel()\n        fpr = fp / (fp + tn)\n        results[3].append(fpr)\n\n# predictions for each group\nfor i in range(1,8):\n    if i != 4:\n        results[4].append(y_hat[group_test == i].mean())\n\nfor i in range(5):\n    datf[columns[i]] = results[i]\ndatf.head()\n\nTotal accuracy: 0.8294004420047052\nPositive predictive value: 0.7998197734294542\nFalse negative rate: 0.11884839029924833\nFalse positive rate: 0.22290711009174313\n\n\n\n\n\n\n  \n    \n      \n      Accuracy\n      PPV\n      FNR\n      FPR\n      Prediction Rate\n    \n  \n  \n    \n      White\n      0.830756\n      0.806017\n      0.119630\n      0.220993\n      0.557623\n    \n    \n      Black\n      0.822704\n      0.792363\n      0.135417\n      0.217500\n      0.534439\n    \n    \n      American Indian\n      0.882353\n      0.750000\n      0.000000\n      0.181818\n      0.470588\n    \n    \n      American Indian and Alaska Native\n      0.666667\n      0.750000\n      0.571429\n      0.125000\n      0.266667\n    \n    \n      Asian\n      0.813928\n      0.758095\n      0.099548\n      0.266247\n      0.571273\n    \n  \n\n\n\n\nThere was only one observation for Alaska Native group, so it was removed from the results.\n\nBias Measures\nTo be calibrated, the score threshold should be the same for all samples in the dataset. Since that is the case here, my model is calibrated.\nThe three largest groups consisting of white, black, and asian individuals have similar FNR at ~11±2% and FPR at ~22% (with asian individuals at ~27%), however most of the other groups that have non-zero results have a much higher FNR and lower FPR. This means that there is some error rate imbalance in my model.\nMost of my predictions rates are within 10% except for American Indian and Alaska Native. This means that my model does not satisfy statistical parity.\n\n\nConcluding Discussion\nThe groups of people that could benefit from a system that is able to predict the label I predicted are people who are looking for a job, and companies that are looking to hire people. This model could be used to predict whether or not someone is employed, and if they are not, it could be used to predict whether or not they will be employed in the future. This could be used by companies to determine whether or not they should hire someone, and it could be used by people to determine whether or not they should apply for a job.\nDeploying this model for large-scale prediction in commercial or governmental settings could lead to a lot of people not getting jobs that they should have gotten, and a lot of people getting jobs that they should not have gotten. This could lead to a lot of people being unemployed that should not be, and a lot of people being employed that should not be. This could lead to a lot of people being unhappy, and a lot of companies being unhappy. This is exarcerbated for American Indian and Alaska Native who have a very high FNR (57%) and very low FPR (12%).\nMy model definitely displays problematic bias. Although it is calibrated, it has error rate imbalance and does not satisfy statistical parity.\nOutside of the bias, I think that this model could be used to discriminate against people who are unemployed. I think that this could be addressed by making sure that the model is only used to predict whether or not someone will be employed in the future."
  }
]