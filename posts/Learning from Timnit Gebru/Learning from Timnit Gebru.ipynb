{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Blog Post 8: Learning from Timnit Gebru\"\n",
    "author: Zayn Makdessi\n",
    "date: '2023-04-19'\n",
    "description: \"Part 1\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Monday April 24th 2023, Dr. Timnit Gebru is virtually visiting our class and giving a talk to the campus. Dr. Gebru is a computer scientist and researcher who has become a leading voice in the field of artificial intelligence and its ethical implications. She is widely recognized for her research on algorithmic bias, specifically the ways in which machine learning models can perpetuate and amplify social inequalities. Dr. Gebru's work has highlighted the need for greater transparency and accountability in the development and deployment of AI systems, as well as the importance of diversity and inclusion in the tech industry. Her contributions to the field have been recognized with numerous awards and honors, including being named one of Time Magazine's 100 most influential people in the world in 2021. Dr. Gebru's advocacy and activism have helped to raise awareness of the ethical concerns surrounding AI and have sparked important discussions about the role of technology in shaping society.\n",
    "\n",
    "Dr. Gebru discusses the limitations of computer vision in identifying emotions and the potential harms of using facial recognition technology, especially in policing. She argues that we need to consider the implications and potential biases of these technologies before they are implemented, and that we must remember that on both sides of data collection, there are human beings involved. Dr. Gebru then gives an example from her own work called \"Gender Shades,\" which was released in 2018. The study found a high disparity in error rates among lighter-skinned men and darker-skinned women on APIs with automated facial analysis tools. Specifically, the study analyzed gender classification tools that classify a person in a picture as male or female, and found that the results were not accurate, especially for people with darker skin. Dr. Gebru also discusses the challenges of using race as a category, as it is a social construct that is unstable across time and space. Dr. Gebru and her team had to create their own data set for the Gender Shades study to ensure that it was more balanced by skin type and gender for their analysis.\n",
    "\n",
    "A variety of issues with bias and lack of diversity in data sets used in computer vision are then discussed, as well as how this can result in problems such as inaccurate object recognition and biased algorithms. She mentions examples such as how crash tests for cars were based on prototypical male characteristics, resulting in cars that disproportionately harm women and children, and how clinical trials were not tested on women, resulting in drugs that disproportionately harmed them. Dr. Gebru notes that there has been more discussion on fairness, accountability, transparency, and ethics in computer vision recently, but that there is still a lack of understanding on how social structures and problems can affect these issues. Even with a drive to make data-driven systems fairer, such as by having more diverse data sets, there is often not enough consideration of how these systems are actually being used and how they affect the most vulnerable groups. Dr. Gebru argues that it is important to have structural representation and real representation, beyond just diversity of data sets, on ethics boards and panels.Dr. Gebru also discusses how some companies have attempted to address these issues, but have sometimes gone about it in a harmful or non-consensual way, such as scraping images from YouTube without consent.\n",
    "\n",
    "It's also worrying to hear that flawed facial recognition is being used as evidence in court, which could result in innocent people being wrongly convicted. The issue of automation bias is also important to consider, as it could lead to people blindly following the recommendations of these tools without questioning their accuracy or reliability. It's clear that more research and action is needed to ensure that facial recognition technology is used in a responsible and ethical manner, and that it doesn't lead to the infringement of people's civil liberties or the targeting of certain groups based on their race, ethnicity, or other characteristics. \n",
    "\n",
    "tl;dr There are many problems with computer vision and facial recognition technology, particularly in how it perpetuates systemic bias and marginalization of vulnerable groups, so we need engineers and scientists to understand the societal implications of their work, and that they should probably prioritize ethics, fairness, accountability, and transparency in their work, and give frameworks to think about these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the pervasive use of computer vision technologies to marginalize vulnerable communities, what do you believe are the most urgent steps that engineers and scientists can take to ensure that their work promotes fairness and accountability, rather than perpetuating existing power imbalances?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('3.8.11')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb72b2de8ba6122566281cdba76e24a805bd33bd16a12cc8fcf1a6467ec304f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
